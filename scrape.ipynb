{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e3c1ba",
   "metadata": {},
   "source": [
    "# LinkedIn Market Analysis\n",
    "\n",
    "\n",
    "### Process:\n",
    "1. Scrape data from Linkedin and Glassdoor, using Selenium.\n",
    "2. EDA, cleaning, and export to CSV.\n",
    "3. Compare to previous years' data (2 and 4 years ago) using tableau.\n",
    "\n",
    "### Additional Datasets:\n",
    "- 2018: \n",
    "- 2020: \n",
    "- 2022: [my submission to Kaggle]\n",
    "\n",
    "### Reference Notebooks:\n",
    "- https://www.kaggle.com/code/gawainlai/us-data-science-job-salary-regression-w-visuals (beyond my skill level)\n",
    "- https://www.kaggle.com/code/discdiver/the-most-in-demand-skills-for-data-scientists (top skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafd57a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ca6211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# web scraping imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# database imports\n",
    "import re as re\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "\n",
    "# import and load file to login to LinkedIn\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3331f3",
   "metadata": {},
   "source": [
    "## Scrape LinkedIn Using the Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a23b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/4xsyrpbs305132t_87mnd3600000gn/T/ipykernel_74274/3666220930.py:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(options = options, executable_path='./chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# initialize the LinkedIn scrape\n",
    "\n",
    "# Options\n",
    "options = webdriver.ChromeOptions() # init for chrome\n",
    "options.add_argument('--incognito') # runs chrome in a 'clean slate' window\n",
    "#options.add_argument('--headless') # runs chromedriver in the background, without opening a window\n",
    "\n",
    "# Initialize the selenium driver\n",
    "driver = webdriver.Chrome(options = options, executable_path='./chromedriver')\n",
    "login_url = \"https://www.linkedin.com/uas/login\"\n",
    "\n",
    "# Start the page\n",
    "driver.get(login_url)\n",
    "time.sleep(3)\n",
    "\n",
    "# Target the login elements\n",
    "email = driver.find_element(\"id\", \"username\")\n",
    "password = driver.find_element(\"id\", \"password\")\n",
    "\n",
    "# Load env variables\n",
    "my_email = os.getenv(\"linkedin_username\")\n",
    "my_password = os.getenv(\"linkedin_password\")\n",
    "\n",
    "# Input in the form\n",
    "email.send_keys(my_email)\n",
    "password.send_keys(my_password)\n",
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ba4ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def scrape(data_role):\\n     Scrape 40 pages of LinkedIn job search, for the given data role \\n    \\n    # create lists to store all scraped data (12 criteria)\\n    search, titles, companies, locations, remote, post_dates, num_applicants, full_time, size, easy_apply, desc, salaries = [], [], [], [], [], [], [], [], [], [], [], []\\n        \\n    for i in range(2): # FIX: change to 40. scrape jobs until page=40\\n        print(i+1, \\'of 2 pages.\\')\\n        \\n        # navigate to the correct page\\n        scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&refresh=true&start={i*25}\"\\n        if i == 0:\\n            scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&refresh=true&start={1}\"\\n        driver.get(scrape_url)\\n        time.sleep(5)\\n\\n        # SCROLL\\n        \\n        \\n        time.sleep(10)\\n        \\n        # increment to next page\\n        #driver.find_element(\"xpath\", f\\'//button[@aria-label=\"Page {i}\"]\\').click()\\n        if i < 9:\\n            driver.find_element(by=By.XPATH, value=\\n                f\\'/html/body/div[6]/div[3]/div[4]/div/div/main/div/section[1]/div/div[6]/ul/li[{i+1}]/button\\').click()\\n                  /html/body/div[6]/div[3]/div[4]/div/div/main/div/section[1]/div/div[6]/ul/li[1]/button\\n        elif i < 33:\\n            driver.find_element(\"xpath\", \\n                \\'/html/body/div[6]/div[3]/div[4]/div/div/main/div/section[1]/div/div[6]/ul/li[7]/button\\').click()\\n        else:\\n            driver.find_element(\"xpath\", \\n                f\\'/html/body/div[6]/div[3]/div[4]/div/div/main/div/section[1]/div/div[6]/ul/li[{i-28}]/button\\').click()\\n        \\n        \\n        # convert page text to beautiful soup\\n        src = driver.page_source\\n        soup_for_page = BeautifulSoup(src, \\'lxml\\')\\n        \\n        # create a list of jobs on the current page, to iterate through after each scrape\\n        job_list = []\\n        jobs_on_page = soup_for_page.find_all(\"a\", attrs={\"class\":\"disabled ember-view job-card-container__link job-card-list__title\"})\\n        for i in jobs_on_page: # length of jobs varies by page\\n            job_list.append(i[\"href\"])\\n            print(i[\"href\"])\\n        print(\\'length of job list: \\', len(job_list)) # FIX: debugging\\n        # print(job_list) # FIX: debugging\\n            \\n        # scrape all jobs on the current page\\n        for i in range(len(job_list)):\\n            #print(\\'index of job list while looping: \\', i)\\n            \\n            \\n            WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"a[href=/jobs/view/3180134475/?eBP=CwEAAAGC1WAJO4bYPFukittYtSpBObZh0R9VanwII8SRPftbBrtLOH6kpCOuTagUlr1pu3vL9UUPR6WM3Sk8gmGLXHpoQ9527cecGaIauJjkEx3mw5ROl9WABoi-WDn2IYryS0W_kfex8vmXNwsq2VdHxj1TSEnidM_OLEE0Z_Almjae3-7MW2R9e-y9_ZsPvW5tnCWfoqq9Eq7kTsgVggwGelIvII2CMn0zs9EV7dkLFdfRekr7II9igjjFrsikEHD9RKZLVHuBc6GTjEnZVkT2nPbt_cns80xGxSyUt1Ozk1glHLeLMC6xe-HuQIALB2t-moR3cRdCGHGX5J3UF-YW&recommendedFlavor=ACTIVELY_HIRING_COMPANY&refId=8ZHQ5KpI5pIlGSwHIyNoyg%3D%3D&trackingId=gnVgCfpSBh21IA2z06Cyaw%3D%3D&trk=flagship3_search_srp_jobs/]\"))).click()\\n            #driver.find_element(By.CSS_SELECTOR, \"disabled ember-view job-card-container__link job-card-list__title\").click()\\n            # ALL ATTEMPS TO CLICK ON A JOB WITH SELENIUM: use selenium to click on next job\\n            #driver.find_element(job_list[i]).click() #also try elements\\n            #driver.find_element(By.linkText(job_list[0])).Click();\\n            #link = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.class, \\'\"disabled ember-view job-card-container__link job-card-list__title\"\\')))\\n            #ActionChains(driver).move_to_element(job_list[0]).click().perform()\\n            #driver.find_element(By.)\\n            #driver.findElement(By.xpath(\"//a[@href=\"\" + job_list[0] + \"\"]\")).Click() #for xpath\\n            #driver.find_element_by_css_selector(f\\'[href^=http://linkedin.com/{job_list[0]}]\\').Click() # for links\\n            #driver.find_element(By.CSS_SELECTOR, f\\'[href^={job_list[0]}]\\').Click() # for links\\n            #driver.find_element(By.XPATH, f\\'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{2}]/div/div/div[1]/div[2]/div[1]/a\\').click()\\n            \\n\\n            #time.sleep(5)\\n            \\n            # convert page text to beautiful soup\\n            src = driver.page_source\\n            soup = BeautifulSoup(src, \\'lxml\\') # FIX: rename soup_for_job and replace all var names\\n            \\n            \\n            \\n            # Return data of results on current selected sub-page to new lists\\n            #time.sleep(5)\\n            titles.append(soup.select(\\'h2.t-24.t-bold.jobs-unified-top-card__job-title\\')[0].get_text().replace(\\'\\n\\',\\'\\').strip())\\n            companies.append(soup.select(\\'span.jobs-unified-top-card__company-name\\')[0].get_text().replace(\\'\\n\\',\\'\\').strip())\\n            locations.append(soup.select(\\'span.jobs-unified-top-card__bullet\\')[1].get_text().replace(\\'\\n\\',\\'\\').strip())\\n            remote.append(soup.select(\\'span.jobs-unified-top-card__workplace-type\\')[0].get_text().replace(\\'\\n\\',\\'\\').strip())\\n            \\n            try:\\n                post_dates.append(soup.select(\\'span.jobs-unified-top-card__posted-date\\')[0].get_text().replace(\\'\\n\\',\\'\\').strip())\\n            except:\\n                print(\"could not find \\'posted date\\'\")\\n                post_dates.append(None)\\n            \\n            try:\\n                num_applicants.append(soup.select(\\'span.jobs-unified-top-card__bullet\\')[2].get_text().replace(\\'\\n\\',\\'\\').strip())\\n            except:\\n                print(\"could not find \\'number of applicants\\'\")\\n                num_applicants.append(None)\\n            \\n            try:\\n                full_time.append(soup.select(\\'li.jobs-unified-top-card__job-insight\\')[0].get_text()\\n                    .replace(\\'\\n\\',\\'\\').strip().rsplit(\\' \\', 1)[-1])\\n            except:\\n                print(\"could not find \\'full time\\'\")\\n                full_time.append(None)\\n            \\n            #time.sleep(5)\\n            \\n            try:\\n                print(soup.select(\\'li.jobs-unified-top-card__job-insight\\')[3].get_text())\\n                size.append(soup.select(\\'li.jobs-unified-top-card__job-insight\\')[3].get_text().replace(\\'\\n\\',\\'\\').strip())\\n            except:\\n                print(\"could not find \\'company size\\'\")\\n                size.append(None)\\n            \\n            try:\\n                print(soup.select(\\'span.artdeco-button__text\\')[42].get_text())\\n                easy_apply.append(soup.select(\\'span.artdeco-button__text\\')[42].get_text())\\n                #alt: div.jobs-apply-button--top-card\\n            except:\\n                print(\"could not find \\'easy apply button\\'\")\\n                easy_apply.append(None)\\n            \\n            desc.append(soup.select(\\'div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch\\')[0].get_text().replace(\\'\\n\\',\\' \\').strip())\\n            \\n            try:\\n                salaries.append(soup.select(\\'p.t-16\\')[0].get_text().replace(\\'\\n\\',\\'\\').strip())\\n            except:\\n                print(\"could not find \\'salary\\' on page\")\\n                try:\\n                    salaries.append(re.find(\\'($.)\\', desc).replace(\\'\\n\\',\\'\\').strip())\\n                except:\\n                    print(\"could not find \\'salary\\' in description\")\\n                    try: \\n                        salaries.append(soup.select(\\'li.jobs-unified-top-card__job-insight\\')[0].get_text()\\n                            .replace(\\'\\n\\',\\'\\').strip().rsplit(\\' \\', 1)[0].rstrip(\\' ·\\'))\\n                    except:\\n                        print(\"could not find \\'salary\\' in full-time\")\\n                        salaries.append(None)\\n            \\n            search.append(data_role.replace(\\'%20\\',\\' \\'))\\n    \\n    \\n    # pass data from lists into a dictionary\\n    dict_from_scrape = {\\'search\\':search, \\'title\\':titles, \\'company\\':companies, \\'location\\':locations, \\'post_date\\':post_dates, \\n                        \\'num_applicants\\':num_applicants, \\'full_time\\':full_time, \\'size\\':size, \\n                        \\'easy_apply\\':easy_apply, \\'description\\':desc, \\'salary\\':salaries}\\n    \\n    df_from_scrape = pd.DataFrame(dict_from_scrape)\\n    \\n    return df_from_scrape\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def scrape(data_role):\n",
    "    \"\"\"\"\"\" Scrape 40 pages of LinkedIn job search, for the given data role \"\"\"\"\"\"\n",
    "    \n",
    "    # create lists to store all scraped data (12 criteria)\n",
    "    search, titles, companies, locations, remote, post_dates, num_applicants, full_time, size, easy_apply, desc, salaries = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "        \n",
    "    for i in range(2): # FIX: change to 40. scrape jobs until page=40\n",
    "        print(i+1, 'of 2 pages.')\n",
    "        \n",
    "        # navigate to the correct page\n",
    "        scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&refresh=true&start={i*25}\"\n",
    "        if i == 0:\n",
    "            scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&refresh=true&start={1}\"\n",
    "        driver.get(scrape_url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # SCROLL\n",
    "        \n",
    "        \n",
    "        time.sleep(10)\n",
    "        \"\"\"\"\"\"\n",
    "        # increment to next page\n",
    "        #driver.find_element(\"xpath\", f'//button[@aria-label=\"Page {i}\"]').click()\n",
    "        if i < 9:\n",
    "            driver.find_element(by=By.XPATH, value=\n",
    "                f'/html/body/div[6]/div[3]/div[4]/div/div/main/div/section[1]/div/div[6]/ul/li[{i+1}]/button').click()\n",
    "                  /html/body/div[6]/div[3]/div[4]/div/div/main/div/section[1]/div/div[6]/ul/li[1]/button\n",
    "        elif i < 33:\n",
    "            driver.find_element(\"xpath\", \n",
    "                '/html/body/div[6]/div[3]/div[4]/div/div/main/div/section[1]/div/div[6]/ul/li[7]/button').click()\n",
    "        else:\n",
    "            driver.find_element(\"xpath\", \n",
    "                f'/html/body/div[6]/div[3]/div[4]/div/div/main/div/section[1]/div/div[6]/ul/li[{i-28}]/button').click()\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        # convert page text to beautiful soup\n",
    "        src = driver.page_source\n",
    "        soup_for_page = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        # create a list of jobs on the current page, to iterate through after each scrape\n",
    "        job_list = []\n",
    "        jobs_on_page = soup_for_page.find_all(\"a\", attrs={\"class\":\"disabled ember-view job-card-container__link job-card-list__title\"})\n",
    "        for i in jobs_on_page: # length of jobs varies by page\n",
    "            job_list.append(i[\"href\"])\n",
    "            print(i[\"href\"])\n",
    "        print('length of job list: ', len(job_list)) # FIX: debugging\n",
    "        # print(job_list) # FIX: debugging\n",
    "            \n",
    "        # scrape all jobs on the current page\n",
    "        for i in range(len(job_list)):\n",
    "            #print('index of job list while looping: ', i)\n",
    "            \n",
    "            \n",
    "            WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"a[href=/jobs/view/3180134475/?eBP=CwEAAAGC1WAJO4bYPFukittYtSpBObZh0R9VanwII8SRPftbBrtLOH6kpCOuTagUlr1pu3vL9UUPR6WM3Sk8gmGLXHpoQ9527cecGaIauJjkEx3mw5ROl9WABoi-WDn2IYryS0W_kfex8vmXNwsq2VdHxj1TSEnidM_OLEE0Z_Almjae3-7MW2R9e-y9_ZsPvW5tnCWfoqq9Eq7kTsgVggwGelIvII2CMn0zs9EV7dkLFdfRekr7II9igjjFrsikEHD9RKZLVHuBc6GTjEnZVkT2nPbt_cns80xGxSyUt1Ozk1glHLeLMC6xe-HuQIALB2t-moR3cRdCGHGX5J3UF-YW&recommendedFlavor=ACTIVELY_HIRING_COMPANY&refId=8ZHQ5KpI5pIlGSwHIyNoyg%3D%3D&trackingId=gnVgCfpSBh21IA2z06Cyaw%3D%3D&trk=flagship3_search_srp_jobs/]\"))).click()\n",
    "            #driver.find_element(By.CSS_SELECTOR, \"disabled ember-view job-card-container__link job-card-list__title\").click()\n",
    "            # ALL ATTEMPS TO CLICK ON A JOB WITH SELENIUM: use selenium to click on next job\n",
    "            #driver.find_element(job_list[i]).click() #also try elements\n",
    "            #driver.find_element(By.linkText(job_list[0])).Click();\n",
    "            #link = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.class, '\"disabled ember-view job-card-container__link job-card-list__title\"')))\n",
    "            #ActionChains(driver).move_to_element(job_list[0]).click().perform()\n",
    "            #driver.find_element(By.)\n",
    "            #driver.findElement(By.xpath(\"//a[@href=\\\"\" + job_list[0] + \"\\\"]\")).Click() #for xpath\n",
    "            #driver.find_element_by_css_selector(f'[href^=http://linkedin.com/{job_list[0]}]').Click() # for links\n",
    "            #driver.find_element(By.CSS_SELECTOR, f'[href^={job_list[0]}]').Click() # for links\n",
    "            #driver.find_element(By.XPATH, f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{2}]/div/div/div[1]/div[2]/div[1]/a').click()\n",
    "            \n",
    "\n",
    "            #time.sleep(5)\n",
    "            \n",
    "            # convert page text to beautiful soup\n",
    "            src = driver.page_source\n",
    "            soup = BeautifulSoup(src, 'lxml') # FIX: rename soup_for_job and replace all var names\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Return data of results on current selected sub-page to new lists\n",
    "            #time.sleep(5)\n",
    "            titles.append(soup.select('h2.t-24.t-bold.jobs-unified-top-card__job-title')[0].get_text().replace('\\n','').strip())\n",
    "            companies.append(soup.select('span.jobs-unified-top-card__company-name')[0].get_text().replace('\\n','').strip())\n",
    "            locations.append(soup.select('span.jobs-unified-top-card__bullet')[1].get_text().replace('\\n','').strip())\n",
    "            remote.append(soup.select('span.jobs-unified-top-card__workplace-type')[0].get_text().replace('\\n','').strip())\n",
    "            \n",
    "            try:\n",
    "                post_dates.append(soup.select('span.jobs-unified-top-card__posted-date')[0].get_text().replace('\\n','').strip())\n",
    "            except:\n",
    "                print(\"could not find 'posted date'\")\n",
    "                post_dates.append(None)\n",
    "            \n",
    "            try:\n",
    "                num_applicants.append(soup.select('span.jobs-unified-top-card__bullet')[2].get_text().replace('\\n','').strip())\n",
    "            except:\n",
    "                print(\"could not find 'number of applicants'\")\n",
    "                num_applicants.append(None)\n",
    "            \n",
    "            try:\n",
    "                full_time.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                    .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "            except:\n",
    "                print(\"could not find 'full time'\")\n",
    "                full_time.append(None)\n",
    "            \n",
    "            #time.sleep(5)\n",
    "            \n",
    "            try:\n",
    "                print(soup.select('li.jobs-unified-top-card__job-insight')[3].get_text())\n",
    "                size.append(soup.select('li.jobs-unified-top-card__job-insight')[3].get_text().replace('\\n','').strip())\n",
    "            except:\n",
    "                print(\"could not find 'company size'\")\n",
    "                size.append(None)\n",
    "            \n",
    "            try:\n",
    "                print(soup.select('span.artdeco-button__text')[42].get_text())\n",
    "                easy_apply.append(soup.select('span.artdeco-button__text')[42].get_text())\n",
    "                #alt: div.jobs-apply-button--top-card\n",
    "            except:\n",
    "                print(\"could not find 'easy apply button'\")\n",
    "                easy_apply.append(None)\n",
    "            \n",
    "            desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().replace('\\n',' ').strip())\n",
    "            \n",
    "            try:\n",
    "                salaries.append(soup.select('p.t-16')[0].get_text().replace('\\n','').strip())\n",
    "            except:\n",
    "                print(\"could not find 'salary' on page\")\n",
    "                try:\n",
    "                    salaries.append(re.find('($.)', desc).replace('\\n','').strip())\n",
    "                except:\n",
    "                    print(\"could not find 'salary' in description\")\n",
    "                    try: \n",
    "                        salaries.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                            .replace('\\n','').strip().rsplit(' ', 1)[0].rstrip(' ·'))\n",
    "                    except:\n",
    "                        print(\"could not find 'salary' in full-time\")\n",
    "                        salaries.append(None)\n",
    "            \n",
    "            search.append(data_role.replace('%20',' '))\n",
    "    \n",
    "    \n",
    "    # pass data from lists into a dictionary\n",
    "    dict_from_scrape = {'search':search, 'title':titles, 'company':companies, 'location':locations, 'post_date':post_dates, \n",
    "                        'num_applicants':num_applicants, 'full_time':full_time, 'size':size, \n",
    "                        'easy_apply':easy_apply, 'description':desc, 'salary':salaries}\n",
    "    \n",
    "    df_from_scrape = pd.DataFrame(dict_from_scrape)\n",
    "    \n",
    "    return df_from_scrape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1931078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(data_role):\n",
    "    \"\"\" Scrape 40 pages of a LinkedIn job search for job links, using the given data role as the search term \"\"\"\n",
    "    \n",
    "    # SCRAPE 40 PAGES\n",
    "    for i in range(2): # FIX: set 2 to 40 for final product\n",
    "        print(f'running search for {i+1} of 40 pages for {data_role}.')\n",
    "        \n",
    "        # navigate to the correct page\n",
    "        scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&refresh=true&start={i*25}\"\n",
    "        if i == 0:\n",
    "            scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&refresh=true&start={1}\"\n",
    "        driver.get(scrape_url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # SCROLL DOWN THE JOB LIST\n",
    "        #jobs_block = driver.find_element_by_class_name('jobs-search-results__list')\n",
    "        #jobs_list= jobs_block.find_elements(By.CSS_SELECTOR, '.jobs-search-results__list-item')\n",
    "        # scroll down for each job element\n",
    "        #driver.execute_script(\"arguments[0].scrollIntoView();\", job)\n",
    "        #time.sleep(10)\n",
    "        \n",
    "        # convert page text to beautiful soup\n",
    "        src = driver.page_source\n",
    "        soup_for_page = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        # create a list of jobs on the current page, to iterate through after each scrape\n",
    "        job_links = []\n",
    "        jobs_on_page = soup_for_page.find_all(\"a\", attrs={\"class\":\"disabled ember-view job-card-container__link job-card-list__title\"})\n",
    "        for i in jobs_on_page: # length of jobs varies by page\n",
    "            job_links.append(i[\"href\"])\n",
    "        print('length of job links: ', len(job_links)) # FIX: debugging\n",
    "        # print(job_list) # FIX: debugging\n",
    "        \n",
    "    return job_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425be525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listing(links):\n",
    "    \"\"\" Returns all scraped data for each job listing from the links passed into the function \"\"\"\n",
    "\n",
    "    # VARIABLE ASSIGNMENT create lists to store all scraped data (12 criteria)\n",
    "    search, titles, companies, locations, remote, post_dates, num_applicants, full_time, size, easy_apply, desc, salaries = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # scrape all jobs on the current page\n",
    "    for key, value in job_links.items():\n",
    "        \n",
    "        # Navigate to page\n",
    "        print('\\n\\n\\nkey:', key, '\\nvalue:', value[0][0])\n",
    "        driver.get(f'https://linkedin.com{value[0][0]}')\n",
    "        time.sleep(5)\n",
    "        \n",
    "        #click see more\n",
    "        #scroll down??\n",
    "        #time.sleep(5)\n",
    "        \n",
    "        # convert page text to beautiful soup\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        # Return data of results on current selected sub-page to new lists\n",
    "        titles.append(soup.select('h2.t-24.t-bold.jobs-unified-top-card__job-title')[0].get_text().replace('\\n','').strip())\n",
    "        companies.append(soup.select('span.jobs-unified-top-card__company-name')[0].get_text().replace('\\n','').strip())\n",
    "        locations.append(soup.select('span.jobs-unified-top-card__bullet')[1].get_text().replace('\\n','').strip())\n",
    "        remote.append(soup.select('span.jobs-unified-top-card__workplace-type')[0].get_text().replace('\\n','').strip())\n",
    "\n",
    "        try:\n",
    "            post_dates.append(soup.select('span.jobs-unified-top-card__posted-date')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'posted date'\")\n",
    "            post_dates.append(None)\n",
    "\n",
    "        try:\n",
    "            num_applicants.append(soup.select('span.jobs-unified-top-card__bullet')[2].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'number of applicants'\")\n",
    "            num_applicants.append(None)\n",
    "\n",
    "        try:\n",
    "            full_time.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "        except:\n",
    "            print(\"could not find 'full time'\")\n",
    "            full_time.append(None)\n",
    "\n",
    "        try:\n",
    "            print(soup.select('li.jobs-unified-top-card__job-insight')[3].get_text())\n",
    "            size.append(soup.select('li.jobs-unified-top-card__job-insight')[3].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'company size'\")\n",
    "            size.append(None)\n",
    "\n",
    "        try:\n",
    "            print(soup.select('span.artdeco-button__text')[42].get_text())\n",
    "            easy_apply.append(soup.select('span.artdeco-button__text')[42].get_text())\n",
    "            #alt: div.jobs-apply-button--top-card\n",
    "        except:\n",
    "            print(\"could not find 'easy apply button'\")\n",
    "            easy_apply.append(None)\n",
    "\n",
    "        desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().replace('\\n',' ').strip())\n",
    "\n",
    "        try:\n",
    "            salaries.append(soup.select('p.t-16')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'salary' on page\")\n",
    "            try:\n",
    "                salaries.append(re.find('($.)', desc).replace('\\n','').strip())\n",
    "            except:\n",
    "                print(\"could not find 'salary' in description\")\n",
    "                try: \n",
    "                    salaries.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                        .replace('\\n','').strip().rsplit(' ', 1)[0].rstrip(' ·'))\n",
    "                except:\n",
    "                    print(\"could not find 'salary' in full-time\")\n",
    "                    salaries.append(None)\n",
    "\n",
    "        search.append(key)\n",
    "\n",
    "    # pass data from lists into a dictionary\n",
    "    dict_from_scrape = {'search':search, 'title':titles, 'company':companies, 'location':locations, 'post_date':post_dates, \n",
    "                        'num_applicants':num_applicants, 'full_time':full_time, 'size':size, \n",
    "                        'easy_apply':easy_apply, 'description':desc, 'salary':salaries}\n",
    "\n",
    "    df_from_scrape = pd.DataFrame(dict_from_scrape)\n",
    "\n",
    "    return df_from_scrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20270abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running search for 1 of 40 pages for data%20analyst.\n",
      "length of job links:  7\n",
      "running search for 2 of 40 pages for data%20analyst.\n",
      "length of job links:  7\n",
      "running search for 1 of 40 pages for data%20scientist.\n",
      "length of job links:  7\n",
      "running search for 2 of 40 pages for data%20scientist.\n",
      "length of job links:  7\n",
      "{'data analyst': [['/jobs/view/3216076186/?eBP=CwEAAAGC1cAUuRdvomRZiZM3b0_RgXd7rsIErLbReCptjr0I9mHwDnJ7gK4-UjRwKf4QQ0em7vTvHkZrGOXGByszt1zQ9bP0Uahd3tSfU54meROaJ46f9vUZshNYYMvy9ohdy6OaNGnNm2u7BdsWUcVHda2oHcqQxydqVTGWZ8cgwPrISvUMcPuy1olLuYxhcTD9twgJfd2PG2yirgwcm-59-tagEyW8zyBRTM6yCax2WjWvqKa1ye0RC1YG65d31F-dvYIv7Ex30ICKF6AS8a5Wro-vHMvyh_nRTL6lM3XpbP_n8hoFpplhw3cutnKqz5569MRmnDSjc10qbl2boT46frDQYCegTtrlpCSiuLRWdAMNorVMkuo9x7tSRrfZX3g&recommendedFlavor=ACTIVELY_HIRING_COMPANY&refId=Y%2Fne8MG7cAs2JxJC83ZS0g%3D%3D&trackingId=%2FnODt1yXzGrSrxR3n37zGQ%3D%3D&trk=flagship3_search_srp_jobs']], 'data scientist': [['/jobs/view/3141978348/?eBP=CwEAAAGC1cBIMpxhbEhinD9QEiqX4m-2zOyxtZ7gBzEEd39dinOuqRd2s7VMq-hu5og41RyM0KDwxVOir27Mi6GdghfxGuydiXKpu3qJBpFhlONGla1waU5hWdKrYWNgDGdJqu7WvmxYoxnFqhkkKiJKaAgo95MvK2v63gL_0aDLmgtXjFITYQeMukU0ePTAAw6QpZz7nY_4sebpl4eshZFPS79n062o9T4_NjbzedA3JrR2Xlg9qzcxyEqul1wdUfV2gaVtO3Ffym83kLt0XVQfbEVCkiMN-J8RfiRKIs4h_KJV8SVgokm2igWgECkRJZLF13T5tbmkYbjQ3Dv55WfkkuCvLy_tGOc-XAWqCpglGHuawDwGg4riL3fVrqNinvI&recommendedFlavor=ACTIVELY_HIRING_COMPANY&refId=CKrtZDp5ZKxhpgdgD%2FtEVQ%3D%3D&trackingId=u%2BRwuNjwA%2BGN4YHSyxIiNQ%3D%3D&trk=flagship3_search_srp_jobs']]}\n",
      "\n",
      "\n",
      "\n",
      "key: data analyst \n",
      "value: /jobs/view/3216076186/?eBP=CwEAAAGC1cAUuRdvomRZiZM3b0_RgXd7rsIErLbReCptjr0I9mHwDnJ7gK4-UjRwKf4QQ0em7vTvHkZrGOXGByszt1zQ9bP0Uahd3tSfU54meROaJ46f9vUZshNYYMvy9ohdy6OaNGnNm2u7BdsWUcVHda2oHcqQxydqVTGWZ8cgwPrISvUMcPuy1olLuYxhcTD9twgJfd2PG2yirgwcm-59-tagEyW8zyBRTM6yCax2WjWvqKa1ye0RC1YG65d31F-dvYIv7Ex30ICKF6AS8a5Wro-vHMvyh_nRTL6lM3XpbP_n8hoFpplhw3cutnKqz5569MRmnDSjc10qbl2boT46frDQYCegTtrlpCSiuLRWdAMNorVMkuo9x7tSRrfZX3g&recommendedFlavor=ACTIVELY_HIRING_COMPANY&refId=Y%2Fne8MG7cAs2JxJC83ZS0g%3D%3D&trackingId=%2FnODt1yXzGrSrxR3n37zGQ%3D%3D&trk=flagship3_search_srp_jobs\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(job_links) \u001b[38;5;66;03m# FIX: debugging\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# FIX: check for dupliate links\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Scrape each new link\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, \u001b[43mscrape_listing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_links\u001b[49m\u001b[43m)\u001b[49m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m driver\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     16\u001b[0m df\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mscrape_listing\u001b[0;34m(links)\u001b[0m\n\u001b[1;32m     21\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Return data of results on current selected sub-page to new lists\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m titles\u001b[38;5;241m.\u001b[39mappend(\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh2.t-24.t-bold.jobs-unified-top-card__job-title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     25\u001b[0m companies\u001b[38;5;241m.\u001b[39mappend(soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan.jobs-unified-top-card__company-name\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     26\u001b[0m locations\u001b[38;5;241m.\u001b[39mappend(soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan.jobs-unified-top-card__bullet\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip())\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "job_links = {'data analyst':[], 'data scientist':[]} # capture all links to scrape with mathing search term. Titles taken from previous market analysis # FIX: add all roles\n",
    "df = pd.DataFrame() # init dataframe to capture all jobs\n",
    "\n",
    "# for each role title given above, return a list of up to 1,000 jobs from LinkedIn\n",
    "for key, value in job_links.items():\n",
    "    new_list = [scrape_links(key.replace(' ','%20'))]\n",
    "    value.append(new_list)\n",
    "print(job_links) # FIX: debugging\n",
    "\n",
    "# FIX: check for dupliate links\n",
    "\n",
    "# Scrape each new link\n",
    "df = pd.concat([df, scrape_listing(job_links)], axis=0).reset_index(drop=True)\n",
    "driver.close()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47071d8",
   "metadata": {},
   "source": [
    "## Scrape Glassdoor using the Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search glassdoor for all job titles\n",
    "# save to glass_df: job title, salary range (or salary avg? both?)\n",
    "# add glassdoor_salary column to df\n",
    "# fill glassdoor_salary based on role name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7dcea2",
   "metadata": {},
   "source": [
    "## EDA and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# replace null values\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# given previous data sets, check salary info\n",
    "df.salary.unique()\n",
    "\n",
    "# remove duplicates\n",
    "clean.rm_dups(df)\n",
    "\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdba8e",
   "metadata": {},
   "source": [
    "## Export Data to CSV for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a52820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export df to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a0636",
   "metadata": {},
   "source": [
    "## Challenges & Lessons\n",
    "\n",
    "### Text extraction & different libraries: \n",
    "I couldn't extract text from my scraped LinkedIn data because I was try to pass data from one library's format (Selenium) into another library (Beautiful Soup). I restarted my kernel, rewrote my code (a lot), and one solution I found online used a function similar to others I had found. This seems to transfer text into a different format so that it's readable by other libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
