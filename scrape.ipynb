{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e3c1ba",
   "metadata": {},
   "source": [
    "# LinkedIn Market Analysis\n",
    "\n",
    "\n",
    "### Process:\n",
    "1. Scrape data from Linkedin and Glassdoor, using Selenium.\n",
    "2. EDA, cleaning, and export to CSV.\n",
    "3. Compare to previous years' data (2 and 4 years ago) using tableau.\n",
    "\n",
    "### Additional Datasets:\n",
    "- 2018: \n",
    "- 2020: \n",
    "- 2022: [my submission to Kaggle]\n",
    "\n",
    "### Reference Notebooks:\n",
    "- https://www.kaggle.com/code/gawainlai/us-data-science-job-salary-regression-w-visuals (beyond my skill level)\n",
    "- https://www.kaggle.com/code/discdiver/the-most-in-demand-skills-for-data-scientists (top skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafd57a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ca6211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "# web scraping imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# database imports\n",
    "import re as re\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "\n",
    "# import and load file to login to LinkedIn\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3331f3",
   "metadata": {},
   "source": [
    "## Scrape LinkedIn Using the Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a23b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/4xsyrpbs305132t_87mnd3600000gn/T/ipykernel_67510/2181346323.py:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(options = options, executable_path='./chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# initialize the LinkedIn scrape\n",
    "\n",
    "# Options\n",
    "options = webdriver.ChromeOptions() # init for chrome\n",
    "options.add_argument('--incognito') # runs chrome in a 'clean slate' window\n",
    "options.add_argument('--headless') # runs chromedriver in the background, without opening a window\n",
    "\n",
    "# Initialize the selenium driver\n",
    "driver = webdriver.Chrome(options = options, executable_path='./chromedriver')\n",
    "login_url = \"https://www.linkedin.com/uas/login\"\n",
    "\n",
    "# Start the page\n",
    "driver.get(login_url)\n",
    "time.sleep(3)\n",
    "\n",
    "# Target the login elements\n",
    "email = driver.find_element(\"id\", \"username\")\n",
    "password = driver.find_element(\"id\", \"password\")\n",
    "\n",
    "# Load env variables\n",
    "my_email = os.getenv(\"linkedin_username\")\n",
    "my_password = os.getenv(\"linkedin_password\")\n",
    "\n",
    "# Input in the form\n",
    "email.send_keys(my_email)\n",
    "password.send_keys(my_password)\n",
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ba4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(data_role):\n",
    "    \"\"\" Scrape 40 pages of LinkedIn job search, for the given data role \"\"\"\n",
    "    job_num = 1\n",
    "    \n",
    "    # create lists to store all scraped data (12 criteria)\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    remote = []\n",
    "    post_dates = []\n",
    "    num_applicants = []\n",
    "    full_time = []\n",
    "    size = []\n",
    "    easy_apply = []\n",
    "    desc = []\n",
    "    salaries = []\n",
    "    \n",
    "    while job_num < 25: # scrape jobs until page=40 (job_num=975)\n",
    "        # navigate to the correct page\n",
    "        scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&refresh=true&start={job_num}\"\n",
    "        time.sleep(5)\n",
    "        driver.get(scrape_url)\n",
    "\n",
    "        # scrape 25 jobs on the current page\n",
    "        for i in range(1): # FIX: 1 --> 25\n",
    "            \n",
    "            # convert page text to beautiful soup\n",
    "            src = driver.page_source\n",
    "            soup = BeautifulSoup(src, 'lxml')\n",
    "            \n",
    "            # Return data of results on current selected sub-page to new lists\n",
    "            time.sleep(5)\n",
    "            titles.append(soup.select('h2.t-24.t-bold.jobs-unified-top-card__job-title')[0].get_text().replace('\\n','').strip())\n",
    "            companies.append(soup.select('span.jobs-unified-top-card__company-name')[0].get_text().replace('\\n','').strip())\n",
    "            locations.append(soup.select('span.jobs-unified-top-card__bullet')[1].get_text().replace('\\n','').strip())\n",
    "            remote.append(soup.select('span.jobs-unified-top-card__workplace-type')[0].get_text().replace('\\n','').strip())\n",
    "            try:\n",
    "                post_dates.append(soup.select('span.jobs-unified-top-card__posted-date')[0].get_text().replace('\\n','').strip())\n",
    "            except:\n",
    "                print(\"could not find 'posted date'\")\n",
    "                post_dates.append(None)\n",
    "            try:\n",
    "                num_applicants.append(soup.select('span.jobs-unified-top-card__bullet')[2].get_text())\n",
    "            except:\n",
    "                print(\"could not find 'number of applicants'\")\n",
    "                num_applicants.append(None)\n",
    "            try:\n",
    "                full_time.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text())\n",
    "            except:\n",
    "                print(\"could not find 'full time'\")\n",
    "                full_time.append(None)\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                print(soup.select('li.jobs-unified-top-card__job-insight')[3].get_text())\n",
    "                size.append(soup.select('li.jobs-unified-top-card__job-insight')[3].get_text())\n",
    "            except:\n",
    "                print(\"could not find 'company size'\")\n",
    "                size.append(None)\n",
    "            try:\n",
    "                print(soup.select('span.artdeco-button__text')[42].get_text())\n",
    "                easy_apply.append(soup.select('span.artdeco-button__text')[42].get_text())\n",
    "                #alt: div.jobs-apply-button--top-card\n",
    "            except:\n",
    "                print(\"could not find 'easy apply button'\")\n",
    "                easy_apply.append(None)\n",
    "            desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch'))\n",
    "            try:\n",
    "                salaries.append(soup.select('p.t-16')[0].get_text())\n",
    "            except:\n",
    "                print(\"could not find 'salary' on page\")\n",
    "                try: \n",
    "                    salaries.append(re.find('($.)', desc))\n",
    "                except:\n",
    "                    print(\"could not find 'salary' in description\")\n",
    "                    salaries.append(None)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # use selenium to click on next job (in list of 25)\n",
    "            \n",
    "            #\"div#ember606.flex-grow-1.artdeco-entity-lockup__content.ember-view\"\n",
    "            print(soup.select('a#ember608.disabled.ember-view.job-card-container__link.job-card-list__title'))\n",
    "            \n",
    "            #ul = soup.select('ul.scaffold-layout__list-container')[0].find_all('li')\n",
    "            #print(ul)\n",
    "            # seems like ul[0] is the current page, opened\n",
    "            #print('ul index 1: ', ul[1])\n",
    "            #for tag in ul:\n",
    "            #    print('test', tag.text)\n",
    "            \n",
    "            tag = soup.select('a#ember608.disabled.ember-view.job-card-container__link.job-card-list__title')\n",
    "            driver.find_element(tag).click()\n",
    "            \n",
    "            \n",
    "            \n",
    "        # increment to next page\n",
    "        job_num +=25\n",
    "    \n",
    "    # pass data from lists into a dictionary\n",
    "    dict_from_scrape = {'title':titles, 'company':companies, 'location':locations, 'post_date':post_dates, \n",
    "                        'num_applicants':num_applicants, 'full_time':full_time, 'size':size, \n",
    "                        'easy_apply':easy_apply, 'description':desc, 'salary':salaries}\n",
    "    \n",
    "    return dict_from_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20270abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not find 'posted date'\n",
      "could not find 'number of applicants'\n",
      "could not find 'full time'\n",
      "could not find 'company size'\n",
      "could not find 'easy apply button'\n",
      "could not find 'salary' on page\n",
      "could not find 'salary' in description\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': ['Solutions Engineer'],\n",
       " 'company': ['LeapXpert'],\n",
       " 'location': ['New York, United States'],\n",
       " 'post_date': [None],\n",
       " 'num_applicants': [None],\n",
       " 'full_time': [None],\n",
       " 'size': [None],\n",
       " 'easy_apply': [None],\n",
       " 'description': [[<div class=\"jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch\" id=\"job-details\" tabindex=\"-1\">\n",
       "   <!-- -->\n",
       "   <!-- -->\n",
       "   <!-- --> <span>\n",
       "   <!-- --><!-- --> </span>\n",
       "   </div>]],\n",
       " 'salary': [None]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape('data%20analyst')\n",
    "#driver.close()\n",
    "\n",
    "\"\"\"\n",
    "data_roles = ['data analyst', 'data scientist', etc] # taken from previous data analyst analysis\n",
    "\n",
    "for elem in data_roles:\n",
    "    df.append(scrape(elem), axis=1)\n",
    "\n",
    "df\n",
    "\"\"\"\n",
    "scrape('data%20analyst')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47071d8",
   "metadata": {},
   "source": [
    "## Scrape Glassdoor using the Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db0452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search glassdoor for all job titles\n",
    "# save to glass_df: job title, salary range (or salary avg? both?)\n",
    "# add glassdoor_salary column to df\n",
    "# fill glassdoor_salary based on role name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7dcea2",
   "metadata": {},
   "source": [
    "## EDA and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "620c983f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# replace null values\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# given previous data sets, check salary info\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# replace null values\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# given previous data sets, check salary info\n",
    "df.salary.unique()\n",
    "\n",
    "# remove duplicates\n",
    "clean.rm_dups(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdba8e",
   "metadata": {},
   "source": [
    "## Export Data to CSV for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a52820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export df to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a0636",
   "metadata": {},
   "source": [
    "## Challenges & Lessons\n",
    "\n",
    "### Text extraction & different libraries: \n",
    "I couldn't extract text from my scraped LinkedIn data because I was try to pass data from one library's format (Selenium) into another library (Beautiful Soup). I restarted my kernel, rewrote my code (a lot), and one solution I found online used a function similar to others I had found. This seems to transfer text into a different format so that it's readable by other libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
