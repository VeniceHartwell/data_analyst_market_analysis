{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e3c1ba",
   "metadata": {},
   "source": [
    "# LinkedIn Market Analysis\n",
    "\n",
    "\n",
    "### Process:\n",
    "1. Scrape data from Linkedin and Glassdoor, using Selenium.\n",
    "2. EDA, cleaning, and export to CSV.\n",
    "3. Compare to previous years' data (2 and 4 years ago) using tableau.\n",
    "\n",
    "### Additional Datasets:\n",
    "- 2018: https://www.kaggle.com/datasets/discdiver/data-scientist-general-skills-2018-revised (skills specific)\n",
    "- 2020: https://www.kaggle.com/datasets/andrewmvd/data-analyst-jobs (jobs, salary, and location)\n",
    "- 2022: [my submission to Kaggle]\n",
    "\n",
    "### Reference Notebooks:\n",
    "- https://www.kaggle.com/code/gawainlai/us-data-science-job-salary-regression-w-visuals (beyond my skill level)\n",
    "- https://www.kaggle.com/code/discdiver/the-most-in-demand-skills-for-data-scientists (top skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafd57a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ca6211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# web scraping imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# database imports\n",
    "import re as re\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# import and load file to login to LinkedIn\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3331f3",
   "metadata": {},
   "source": [
    "## Scrape LinkedIn Using the Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a23b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/4xsyrpbs305132t_87mnd3600000gn/T/ipykernel_99020/3666220930.py:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(options = options, executable_path='./chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# initialize the LinkedIn scrape\n",
    "\n",
    "# Options\n",
    "options = webdriver.ChromeOptions() # init for chrome\n",
    "options.add_argument('--incognito') # runs chrome in a 'clean slate' window\n",
    "#options.add_argument('--headless') # runs chromedriver in the background, without opening a window\n",
    "\n",
    "# Initialize the selenium driver\n",
    "driver = webdriver.Chrome(options = options, executable_path='./chromedriver')\n",
    "login_url = \"https://www.linkedin.com/uas/login\"\n",
    "\n",
    "# Start the page\n",
    "driver.get(login_url)\n",
    "time.sleep(3)\n",
    "\n",
    "# Target the login elements\n",
    "email = driver.find_element(\"id\", \"username\")\n",
    "password = driver.find_element(\"id\", \"password\")\n",
    "\n",
    "# Load env variables\n",
    "my_email = os.getenv(\"linkedin_username\")\n",
    "my_password = os.getenv(\"linkedin_password\")\n",
    "\n",
    "# Input in the form\n",
    "email.send_keys(my_email)\n",
    "password.send_keys(my_password)\n",
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc63c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(data_role, location):\n",
    "    \"\"\" Scrape 40 pages of a LinkedIn job search for job links, using the given data role as the search term \"\"\"\n",
    "    \n",
    "    # SCRAPE 40 PAGES\n",
    "    for i in range(1): # FIX: change back to 40 for final analysis\n",
    "        print(f'Scraping {i+1} of 40 pages for {data_role} in {location}.')\n",
    "        \n",
    "        # navigate to the correct page\n",
    "        scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&location={location}&refresh=true&start={i*25}\"\n",
    "        # TEST: https://www.linkedin.com/jobs/search/?&keywords=data%20analyst&location=Los%20Angeles%2C%20California%2C%20United%20States&refresh=true&start=1\n",
    "        if i == 0:\n",
    "            scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&location={location}&refresh=true&start={1}\"\n",
    "        driver.get(scrape_url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # convert page text to beautiful soup\n",
    "        src = driver.page_source\n",
    "        soup_for_page = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        # create a list of jobs on the current page, to iterate through after each scrape\n",
    "        job_links = []\n",
    "        jobs_on_page = soup_for_page.find_all(\"a\", attrs={\"class\":\"disabled ember-view job-card-container__link job-card-list__title\"})\n",
    "        for k in jobs_on_page: # length of jobs varies by page\n",
    "            job_links.append(k[\"href\"])\n",
    "        print(f'Job links collected from page {i+1}:', len(job_links)) # DEBUG\n",
    "        #print(f'job links from page {i+1}:',job_links) # DEBUG\n",
    "        \n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7580f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_more():\n",
    "    \"\"\" Load an entire LinkedIn job post page \"\"\"\n",
    "    \n",
    "    # Click on the 'see more' button on a LinkedIn job post (there are several window types)\n",
    "    try:\n",
    "        driver.find_element_by_class_name(\"artdeco-card__actions\").click()\n",
    "        print('find_element_by_class_name(\"artdeco-card__actions\") SUCCESS')\n",
    "    except:\n",
    "        try:\n",
    "            driver.find_element(By.Class, \"show-more-less-html__button show-more-less-html__button--more\").click()\n",
    "            print('driver.find_element(By.Class, \"show-more-less-html__button ... SUCCESS')\n",
    "        except:\n",
    "            print('Find another solution to SEE MORE on this page')\n",
    "                            \n",
    "    # Scroll down the entire page for easier data collection\n",
    "    driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c02a86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listing(links):\n",
    "    \"\"\" Returns all scraped data for each job listing from the links passed into the function \"\"\"\n",
    "\n",
    "    # VARIABLE ASSIGNMENT create lists to store all scraped data (10 criteria)\n",
    "    titles, companies, locations, remote, post_dates, num_applicants, contract, size, desc, salaries = [], \\\n",
    "        [], [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # SCRAPE ALL LINKS scrape all jobs on the current page using passed in links\n",
    "    for idx, link in enumerate(links):\n",
    "        print(f'\\nScraping job {idx} of {len(links)}.') # DEBUG TEXT\n",
    "        \n",
    "        # GO TO PAGE Navigate to page\n",
    "        #print('\\n\\n\\nkey:', key, '\\nvalue:', value[0][0]) # DEBUG TEXT\n",
    "        driver.get(f'https://linkedin.com{link}')\n",
    "        time.sleep(10)\n",
    "        \n",
    "        # SEE FULL PAGE click 'see more' and scroll down\n",
    "        #see_more() # FIX\n",
    "        \n",
    "        # BEAUTFUL SOUP EXTRACTION convert page text to beautiful soup\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        # DATA COLLECTION return data of results on current selected sub-page to new lists\n",
    "        \n",
    "        # TITLE\n",
    "        try:\n",
    "            titles.append(soup.select('h1.t-24.t-bold.jobs-unified-top-card__job-title')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print('title could not be found')\n",
    "            titles.append(None)\n",
    "         \n",
    "        # COMPANY\n",
    "        try:\n",
    "            companies.append(soup.select('span.jobs-unified-top-card__company-name')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print('company could not be found')\n",
    "            companies.append(None)\n",
    "            \n",
    "        # LOCATION\n",
    "        try:\n",
    "            locations.append(soup.select('span.jobs-unified-top-card__bullet')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print('location could not be found')\n",
    "            locations.append(None)\n",
    "        \n",
    "        # REMOTE POSITION\n",
    "        try: # check in header\n",
    "            remote.append(soup.select('span.jobs-unified-top-card__workplace-type')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try: # check the description for remote term.\n",
    "                desc_temp = soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip()\n",
    "                print('remote could not be found in header')\n",
    "                if 'remote' in desc_temp:\n",
    "                    remote.append('remote?')\n",
    "                elif 'hybrid' in desc:\n",
    "                    remote.append('hybrid?')\n",
    "                else:\n",
    "                    remote.append(None)\n",
    "            except:\n",
    "                try: # check in title\n",
    "                    title_temp = soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip()\n",
    "                    if 'remote' in title_temp:\n",
    "                        remote.append('remote?')\n",
    "                    else:\n",
    "                        remote.append(None)\n",
    "                    print('remote position could not be found in header or description')\n",
    "                except:\n",
    "                    print('remote position could not be found in header , description, or title')\n",
    "                    remote.append(None)\n",
    "            \n",
    "        # POST DATE\n",
    "        try:\n",
    "            post_dates.append(soup.select('span.jobs-unified-top-card__posted-date')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'posted date'\")\n",
    "            post_dates.append(None)\n",
    "        \n",
    "        # NUMBER of APPLICANTS\n",
    "        try:\n",
    "            num_applicants.append(soup.select('span.jobs-unified-top-card__applicant-count')[2].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                num_applicants.append(soup.select('span.jobs-unified-top-card__bullet')[1].get_text().replace('\\n','').strip())\n",
    "                print(\"could not find 'number of applicants' in applicant count\")\n",
    "            except:\n",
    "                print(\"could not find 'number of applicants' in applicant count or bullet\")\n",
    "                num_applicants.append(None)\n",
    "        \n",
    "        # FULL TIME\n",
    "        try:\n",
    "            contract.append(soup.select('li.jobs-unified-top-card__job-insight.span').get_text()\n",
    "                .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "        except:\n",
    "            try:\n",
    "                contract.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                    .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "                print('could not find \"contract type\" in job insights.span')\n",
    "            except:\n",
    "                print(\"could not find 'contract type' in job insights.span or job insights\")\n",
    "                contract.append(None)\n",
    "        \n",
    "        # COMPANY SIZE\n",
    "        try:\n",
    "            size.append(soup.select('li.jobs-unified-top-card__job-insight')[1].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'company size'\")\n",
    "            size.append(None)\n",
    "        \n",
    "        # DESCRIPTION\n",
    "        try:\n",
    "            desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip())\n",
    "        except:\n",
    "            \n",
    "            print(\"could not find description (probably shouldn't apply!)\")\n",
    "            \n",
    "        # SALARY\n",
    "        try:\n",
    "            salaries.append(soup.select('a.app-aware-link')[6].get_text().replace('\\n','').strip())\n",
    "            if '$' not in salaries[-1]:\n",
    "                salaries.pop()\n",
    "                salaries.append(None)\n",
    "        except:\n",
    "            try:\n",
    "                salaries.append(soup.select('p.t-16')[0].get_text().replace('\\n','').strip())\n",
    "                print(\"could not find 'salary' with #SALARY tag\")\n",
    "            except:\n",
    "                try:\n",
    "                    salaries.append(re.find('($.)', desc).replace('\\n','').strip())\n",
    "                    print(\"could not find 'salary' with #SALARY tag or in p.t-16\")\n",
    "                except:\n",
    "                    try: \n",
    "                        salaries.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                            .replace('\\n','').strip().rsplit(' ', 1)[0].rstrip(' Â·'))\n",
    "                        print(\"could not find 'salary' with #SALARY tag, in p.t-16, or in description\")\n",
    "                    except:\n",
    "                        print(\"could not find 'salary' with #SALARY tag, in p.t-16, in description, or in full-time\")\n",
    "                        salaries.append(None)\n",
    "\n",
    "    # DICTIONARY ASSIGNMENT pass data from lists into a dictionary\n",
    "    dict_from_scrape = {'title':titles, 'company':companies, 'location':locations, 'remote':remote, \n",
    "                        'post_date':post_dates, 'num_applicants':num_applicants, 'contract_type':contract, \n",
    "                        'company_size':size, 'description':desc, 'salary':salaries}\n",
    "\n",
    "    # DATAFRAME ASSIGNMENT\n",
    "    df_from_scrape = pd.DataFrame(dict_from_scrape)\n",
    "    \n",
    "    os.system(\"say -v Monica ayam don escreipin\")\n",
    "    return df_from_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bde0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_2d_list(list_):\n",
    "    list_flattened = [a for y in list_ for a in y]\n",
    "    return list(set(list_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20270abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for data analyst jobs in San Francisco, California, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in San%20Francisco%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Los Angeles, California, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Los%20Angeles%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in San Jose, California, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in San%20Jose%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in San Diego, California, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in San%20Diego%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Portland, Oregon, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Portland%2C%20Oregon%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Seattle, Washington, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Seattle%2C%20Washington%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Denver, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Denver%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Colorado Springs, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Colorado%20Springs%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Indianapolis, Indiana, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Indianapolis%2C%20Indiana%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in New York, New York, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in New%20York%2C%20New%20York%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Secaucus, New Jersey...\n",
      "Scraping 1 of 40 pages for data%20analyst in Secaucus%2C%20New%20Jersey.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Boston, Massachusetts, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Boston%2C%20Massachusetts%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Baltimore, Maryland, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Baltimore%2C%20Maryland%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Chicago, Illinois, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Chicago%2C%20Illinois%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Philadelphia, Pennsylvania, United StatesPhoenix, Arizona, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Philadelphia%2C%20Pennsylvania%2C%20United%20StatesPhoenix%2C%20Arizona%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Salt Lake City, Utah, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Salt%20Lake%20City%2C%20Utah%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Minneapolis, Minnesota, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Minneapolis%2C%20Minnesota%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Detroit, Michigan, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Detroit%2C%20Michigan%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Columbus, Ohio, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Columbus%2C%20Ohio%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Kansas City, Missouri, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Kansas%20City%2C%20Missouri%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Austin, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Austin%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Dallas, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Dallas%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Houston, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Houston%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Atlanta, Georgia, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Atlanta%2C%20Georgia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Jackson, Mississippi, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Jackson%2C%20Mississippi%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Washington, District of Columbia, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Washington%2C%20District%20of%20Columbia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Charlotte, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Charlotte%2C%20North%20Carolina%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Raleigh, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Raleigh%2C%20North%20Carolina%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Jacksonville, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Jacksonville%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Miami, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Miami%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data analyst jobs in Tampa, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20analyst in Tampa%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in San Francisco, California, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in San%20Francisco%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Los Angeles, California, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Los%20Angeles%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in San Jose, California, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in San%20Jose%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in San Diego, California, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in San%20Diego%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Portland, Oregon, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Portland%2C%20Oregon%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Seattle, Washington, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Seattle%2C%20Washington%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Denver, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Denver%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Colorado Springs, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Colorado%20Springs%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Indianapolis, Indiana, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Indianapolis%2C%20Indiana%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in New York, New York, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in New%20York%2C%20New%20York%2C%20United%20States.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Secaucus, New Jersey...\n",
      "Scraping 1 of 40 pages for data%20scientist in Secaucus%2C%20New%20Jersey.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Boston, Massachusetts, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Boston%2C%20Massachusetts%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Baltimore, Maryland, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Baltimore%2C%20Maryland%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Chicago, Illinois, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Chicago%2C%20Illinois%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Philadelphia, Pennsylvania, United StatesPhoenix, Arizona, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Philadelphia%2C%20Pennsylvania%2C%20United%20StatesPhoenix%2C%20Arizona%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Salt Lake City, Utah, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Salt%20Lake%20City%2C%20Utah%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Minneapolis, Minnesota, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Minneapolis%2C%20Minnesota%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Detroit, Michigan, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Detroit%2C%20Michigan%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Columbus, Ohio, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Columbus%2C%20Ohio%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Kansas City, Missouri, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Kansas%20City%2C%20Missouri%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Austin, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Austin%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Dallas, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Dallas%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Houston, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Houston%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Atlanta, Georgia, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Atlanta%2C%20Georgia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Jackson, Mississippi, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Jackson%2C%20Mississippi%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Washington, District of Columbia, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Washington%2C%20District%20of%20Columbia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Charlotte, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Charlotte%2C%20North%20Carolina%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Raleigh, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Raleigh%2C%20North%20Carolina%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Jacksonville, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Jacksonville%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Miami, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Miami%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data scientist jobs in Tampa, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20scientist in Tampa%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in San Francisco, California, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in San%20Francisco%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Los Angeles, California, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Los%20Angeles%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in San Jose, California, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in San%20Jose%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in San Diego, California, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in San%20Diego%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Portland, Oregon, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Portland%2C%20Oregon%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Seattle, Washington, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Seattle%2C%20Washington%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Denver, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Denver%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Colorado Springs, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Colorado%20Springs%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Indianapolis, Indiana, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Indianapolis%2C%20Indiana%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in New York, New York, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in New%20York%2C%20New%20York%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Secaucus, New Jersey...\n",
      "Scraping 1 of 40 pages for data%20engineer in Secaucus%2C%20New%20Jersey.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Boston, Massachusetts, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Boston%2C%20Massachusetts%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Baltimore, Maryland, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Baltimore%2C%20Maryland%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Chicago, Illinois, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Chicago%2C%20Illinois%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Philadelphia, Pennsylvania, United StatesPhoenix, Arizona, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Philadelphia%2C%20Pennsylvania%2C%20United%20StatesPhoenix%2C%20Arizona%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Salt Lake City, Utah, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Salt%20Lake%20City%2C%20Utah%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Minneapolis, Minnesota, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Minneapolis%2C%20Minnesota%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Detroit, Michigan, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Detroit%2C%20Michigan%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Columbus, Ohio, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Columbus%2C%20Ohio%2C%20United%20States.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Kansas City, Missouri, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Kansas%20City%2C%20Missouri%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Austin, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Austin%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Dallas, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Dallas%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Houston, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Houston%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Atlanta, Georgia, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Atlanta%2C%20Georgia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Jackson, Mississippi, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Jackson%2C%20Mississippi%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Washington, District of Columbia, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Washington%2C%20District%20of%20Columbia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Charlotte, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Charlotte%2C%20North%20Carolina%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Raleigh, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Raleigh%2C%20North%20Carolina%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Jacksonville, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Jacksonville%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Miami, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Miami%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data engineer jobs in Tampa, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20engineer in Tampa%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in San Francisco, California, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in San%20Francisco%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Los Angeles, California, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Los%20Angeles%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in San Jose, California, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in San%20Jose%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in San Diego, California, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in San%20Diego%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Portland, Oregon, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Portland%2C%20Oregon%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Seattle, Washington, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Seattle%2C%20Washington%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Denver, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Denver%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Colorado Springs, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Colorado%20Springs%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Indianapolis, Indiana, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Indianapolis%2C%20Indiana%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in New York, New York, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in New%20York%2C%20New%20York%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Secaucus, New Jersey...\n",
      "Scraping 1 of 40 pages for data%20architect in Secaucus%2C%20New%20Jersey.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Boston, Massachusetts, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Boston%2C%20Massachusetts%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Baltimore, Maryland, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Baltimore%2C%20Maryland%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Chicago, Illinois, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Chicago%2C%20Illinois%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Philadelphia, Pennsylvania, United StatesPhoenix, Arizona, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Philadelphia%2C%20Pennsylvania%2C%20United%20StatesPhoenix%2C%20Arizona%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Salt Lake City, Utah, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Salt%20Lake%20City%2C%20Utah%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Minneapolis, Minnesota, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Minneapolis%2C%20Minnesota%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Detroit, Michigan, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Detroit%2C%20Michigan%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Columbus, Ohio, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Columbus%2C%20Ohio%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Kansas City, Missouri, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Kansas%20City%2C%20Missouri%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Austin, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Austin%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Dallas, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Dallas%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Houston, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Houston%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Atlanta, Georgia, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Atlanta%2C%20Georgia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Jackson, Mississippi, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Jackson%2C%20Mississippi%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Washington, District of Columbia, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Washington%2C%20District%20of%20Columbia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Charlotte, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Charlotte%2C%20North%20Carolina%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Raleigh, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Raleigh%2C%20North%20Carolina%2C%20United%20States.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Jacksonville, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Jacksonville%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Miami, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Miami%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data architect jobs in Tampa, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20architect in Tampa%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in San Francisco, California, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in San%20Francisco%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Los Angeles, California, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Los%20Angeles%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in San Jose, California, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in San%20Jose%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in San Diego, California, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in San%20Diego%2C%20California%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Portland, Oregon, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Portland%2C%20Oregon%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Seattle, Washington, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Seattle%2C%20Washington%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Denver, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Denver%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Colorado Springs, Colorado, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Colorado%20Springs%2C%20Colorado%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Indianapolis, Indiana, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Indianapolis%2C%20Indiana%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in New York, New York, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in New%20York%2C%20New%20York%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Secaucus, New Jersey...\n",
      "Scraping 1 of 40 pages for data%20manager in Secaucus%2C%20New%20Jersey.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Boston, Massachusetts, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Boston%2C%20Massachusetts%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Baltimore, Maryland, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Baltimore%2C%20Maryland%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Chicago, Illinois, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Chicago%2C%20Illinois%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Philadelphia, Pennsylvania, United StatesPhoenix, Arizona, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Philadelphia%2C%20Pennsylvania%2C%20United%20StatesPhoenix%2C%20Arizona%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Salt Lake City, Utah, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Salt%20Lake%20City%2C%20Utah%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Minneapolis, Minnesota, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Minneapolis%2C%20Minnesota%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Detroit, Michigan, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Detroit%2C%20Michigan%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Columbus, Ohio, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Columbus%2C%20Ohio%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Kansas City, Missouri, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Kansas%20City%2C%20Missouri%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Austin, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Austin%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Dallas, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Dallas%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Houston, Texas, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Houston%2C%20Texas%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Atlanta, Georgia, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Atlanta%2C%20Georgia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Jackson, Mississippi, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Jackson%2C%20Mississippi%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Washington, District of Columbia, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Washington%2C%20District%20of%20Columbia%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Charlotte, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Charlotte%2C%20North%20Carolina%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Raleigh, North Carolina, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Raleigh%2C%20North%20Carolina%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Jacksonville, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Jacksonville%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Miami, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Miami%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n",
      "Searching for data manager jobs in Tampa, Florida, United States...\n",
      "Scraping 1 of 40 pages for data%20manager in Tampa%2C%20Florida%2C%20United%20States.\n",
      "Job links collected from page 1: 7\n"
     ]
    }
   ],
   "source": [
    "# LISTS FOR SCRAPING\n",
    "\n",
    "# 5 titles taken from market analysis, used to capture all links to scrape with matching search term.\n",
    "data_roles = ['data analyst','data scientist','data engineer','data architect','data manager']\n",
    "# removed from final scrape to reduce noise and risk of account ban: 'finance analyst','data warehouse analyst','data manager','data marketing analyst'\n",
    "\n",
    "# 32 locations chosen from top tech cities across US (excluding search results yielding the same listings on LinkedIn)\n",
    "locations = ['San Francisco, California, United States','Los Angeles, California, United States','San Jose, California, United States',\n",
    "             'San Diego, California, United States','Portland, Oregon, United States','Seattle, Washington, United States',\n",
    "             'Denver, Colorado, United States', 'Colorado Springs, Colorado, United States','Indianapolis, Indiana, United States',\n",
    "             'New York, New York, United States','Secaucus, New Jersey', 'Boston, Massachusetts, United States', \n",
    "             'Baltimore, Maryland, United States','Chicago, Illinois, United States','Philadelphia, Pennsylvania, United States'\n",
    "             'Phoenix, Arizona, United States','Salt Lake City, Utah, United States','Minneapolis, Minnesota, United States',\n",
    "             'Detroit, Michigan, United States','Columbus, Ohio, United States','Kansas City, Missouri, United States',\n",
    "             'Austin, Texas, United States','Dallas, Texas, United States','Houston, Texas, United States', \n",
    "             'Atlanta, Georgia, United States','Jackson, Mississippi, United States','Washington, District of Columbia, United States',\n",
    "             'Charlotte, North Carolina, United States','Raleigh, North Carolina, United States',\n",
    "             'Jacksonville, Florida, United States','Miami, Florida, United States','Tampa, Florida, United States']\n",
    "\n",
    "\"\"\"\n",
    "HOW MUCH DATA IS ENOUGH ?\n",
    "35 locations * 9 titles * 2 pages = 630 pages. At the full 40 pages, the real total of my scrape will be \n",
    "12,600 pages. Assuming I don't get banned for scraping 10,000 pages, let alone 100 pages, I will still need \n",
    "to scrape the links that come from them. That's 630 pages * 7 links = 4,410 links for the sample and \n",
    "12,600 * 25 = 315,000 links for the real scrape. Of course, most jobs will be duplicates, but that doesn't \n",
    "change that I will have to wait a long time for data and I may get banned several times before the scrape is\n",
    "complete. In reality, it may be safer to limit my searches to fewer titles, locations, pages and links.\n",
    "\n",
    "The new scrape of 5 roles * 32 cities * 40 pages = 6,400 (or 320 for the 2-page sample) is more reasonable.\n",
    "\n",
    "state_locations = ['Washington', 'California', 'Colorado', 'Texas', 'Illinois', 'Florida', 'Atlanta', 'New York']\n",
    "\n",
    "global_locations = [Barcelona, Madrid, Berlin, Munich, Amsterdam, London, Dublin, Stockholm, Copenhagen, Oslo,\n",
    "             Luxembourg, Eindhoven, Manchester, Belfast, Bristol, Paris, Budapest, Bucharest, Warsaw, Prague, \n",
    "             Lisbon, Rome, Zurich, vancouver, ontario, montreal, toronto, \n",
    "             Melbourne, Moscow, Seoule, Jakarta, Kyiv, tokyo, rejkjavik,\n",
    "             argentina, mexico city, lima, rio, buenos aires, sao paolo, panama,] \n",
    "\"\"\"\n",
    "\n",
    "job_links = [] # init list to capture all job links\n",
    "\n",
    "# SCRAPE search LinkedIn for each role title and location given above and return a list of up to 1,000 jobs\n",
    "for title in data_roles:\n",
    "    for location in locations:\n",
    "        print(f'Searching for {title} jobs in {location}...')\n",
    "        job_links.append(scrape_links(title.replace(' ','%20'), location.replace(',','%2C').replace(' ','%20')))\n",
    "#print(job_links) # DEBUG\n",
    "\n",
    "# DUPLICATES remove dupliate links and flatten 2d array before scraping\n",
    "job_links_cleaned = flatten_2d_list(job_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0cf97cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1893671553.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [61]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print('title could not be found')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "def scrape_listing_manually(links):\n",
    "    \"\"\" Returns all scraped data for each job listing from the links passed into the function \"\"\"\n",
    "\n",
    "    # VARIABLE ASSIGNMENT create lists to store all scraped data (10 criteria)\n",
    "    titles, companies, locations, remote, post_dates, num_applicants, contract, size, desc, salaries = [], \\\n",
    "        [], [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # SCRAPE ALL LINKS scrape all jobs on the current page using passed in links\n",
    "    for idx, link in enumerate(links[:2]):\n",
    "        print(f'\\nScraping job {idx+1} of {len(links)}.\\n') # DEBUG TEXT\n",
    "        print(f'https://linkedin.com{link}\\n') # DEBUG TEXT\n",
    "        #opener = urllib.request.FancyURLopener({})\n",
    "        opener = urllib.request.urlopen(f'https://linkedin.com{link}')\n",
    "        #with opener.open(f'https://linkedin.com{link}') as f: \n",
    "        #    f.read().decode('utf-8')\n",
    "            #content = f.read()\n",
    "        #with open('data_page.html', 'r') as f:\n",
    "            #contents = f.read()\n",
    "        soup = BeautifulSoup(opener, 'lxml')\n",
    "        #print('empty soup', soup)\n",
    "\n",
    "        # DATA COLLECTION return data of results on current selected sub-page to new lists\n",
    "        # TITLE\n",
    "        try:\n",
    "            titles.append(soup.select('h1.t-24.t-bold.jobs-unified-top-card__job-title')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                titles.append(soup.select('h1.top-card-layout__title.font-sans.text-lg.papabear:text-xl.font-bold.leading-open.text-color-text.mb-0.topcard__title').get_text().strip())\n",
    "            except:\n",
    "                print('title could not be found')\n",
    "                titles.append(None)\n",
    "\n",
    "        # COMPANY\n",
    "        try:\n",
    "            companies.append(soup.select('span.jobs-unified-top-card__company-name')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                companies.append(soup.select('a.topcard__org-name-link.topcard__flavor--black-link').get_text().strip())\n",
    "            except:    \n",
    "                print('company could not be found')\n",
    "                companies.append(None)\n",
    "\n",
    "        # LOCATION\n",
    "        try:\n",
    "            locations.append(soup.select('span.jobs-unified-top-card__bullet')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                locations.append(soup.select('span.topcard__flavor.topcard__flavor').get_text().strip())\n",
    "            except: \n",
    "                print('location could not be found')\n",
    "                locations.append(None)\n",
    "\n",
    "        # REMOTE POSITION\n",
    "        try: # check in header\n",
    "            remote.append(soup.select('span.jobs-unified-top-card__workplace-type')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try: # check the description for remote term.\n",
    "                desc_temp = soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip()\n",
    "                print('remote could not be found in header')\n",
    "                if 'remote' in desc_temp:\n",
    "                    remote.append('remote?')\n",
    "                elif 'hybrid' in desc:\n",
    "                    remote.append('hybrid?')\n",
    "                else:\n",
    "                    remote.append(None)\n",
    "            except:\n",
    "                try: # check in title\n",
    "                    title_temp = soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip()\n",
    "                    if 'remote' in title_temp:\n",
    "                        remote.append('remote?')\n",
    "                    else:\n",
    "                        remote.append(None)\n",
    "                    print('remote position could not be found in header or description')\n",
    "                except:\n",
    "                    print('remote position could not be found in header , description, or title')\n",
    "                    remote.append(None)\n",
    "\n",
    "        # POST DATE\n",
    "        try:\n",
    "            post_dates.append(soup.select('span.jobs-unified-top-card__posted-date')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"â could not find 'posted date'\")\n",
    "            post_dates.append(None)\n",
    "\n",
    "        # NUMBER of APPLICANTS\n",
    "        try:\n",
    "            num_applicants.append(soup.select('span.jobs-unified-top-card__applicant-count')[2].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                num_applicants.append(soup.select('span.jobs-unified-top-card__bullet')[1].get_text().replace('\\n','').strip())\n",
    "                print(\"could not find 'number of applicants' in applicant count\")\n",
    "            except:\n",
    "                try:\n",
    "                    num_applicants.append(soup.select('num_applicants__caption').get_text().replace('\\n','').strip())\n",
    "                    print(\"could not find 'number of applicants' in applicant count or bullet\")\n",
    "                except:\n",
    "                    print(\"â could not find 'number of applicants' in applicant count, bullet, or caption\")\n",
    "                    num_applicants.append(None)\n",
    "\n",
    "        # FULL TIME\n",
    "        try:\n",
    "            contract.append(soup.select('li.jobs-unified-top-card__job-insight.span').get_text()\n",
    "                .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "        except:\n",
    "            try:\n",
    "                contract.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                    .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "                print('could not find \"contract type\" in job insights.span')\n",
    "            except:\n",
    "                print(\"â could not find 'contract type' in job insights.span or job insights\")\n",
    "                contract.append(None)\n",
    "\n",
    "        # COMPANY SIZE\n",
    "        try:\n",
    "            size.append(soup.select('li.jobs-unified-top-card__job-insight')[1].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"â could not find 'company size'\")\n",
    "            size.append(None)\n",
    "\n",
    "        # DESCRIPTION\n",
    "        try:\n",
    "            desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip())\n",
    "        except:\n",
    "            try:\n",
    "                desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch.span').get_text().strip())\n",
    "            except:\n",
    "                try:\n",
    "                    desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch').get_text().strip())\n",
    "                except:\n",
    "                    try:\n",
    "                        desc.append(soup.select('div.job-details.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip())\n",
    "                    except:\n",
    "                        try:\n",
    "                            print('text rich\\n', soup.select('div.description__text.description__text--rich')[0].get_text().strip())\n",
    "                            desc.append(soup.select('div.description__text.description__text--rich')[0].get_text().strip())\n",
    "                        except:\n",
    "                            try:\n",
    "                                desc.append(soup.select('div.show-more-less-html__markup.show-more-less-html__markup--clamp-after-5')[0])\n",
    "                                print('clamp after 5\\n', soup.select('div.show-more-less-html__markup.show-more-less-html__markup--clamp-after-5')[0])\n",
    "                            except:\n",
    "                                desc.append(None)\n",
    "                                print(\"â ï¸ could not find description.\")\n",
    "\n",
    "        # SALARY\n",
    "        try:\n",
    "            salaries.append(soup.select('a.app-aware-link')[6].get_text().replace('\\n','').strip())\n",
    "            if '$' not in salaries[-1]:\n",
    "                salaries.pop()\n",
    "                salaries.append(None)\n",
    "        except:\n",
    "            try:\n",
    "                salaries.append(soup.select('p.t-16')[0].get_text().replace('\\n','').strip())\n",
    "                print(\"could not find 'salary' with #SALARY tag\")\n",
    "            except:\n",
    "                try:\n",
    "                    salaries.append(re.find('($.)', desc).replace('\\n','').strip())\n",
    "                    print(\"could not find 'salary' with #SALARY tag or in p.t-16\")\n",
    "                except:\n",
    "                    try: \n",
    "                        salaries.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                            .replace('\\n','').strip().rsplit(' ', 1)[0].rstrip(' Â·'))\n",
    "                        print(\"could not find 'salary' with #SALARY tag, in p.t-16, or in description\")\n",
    "                    except:\n",
    "                        print(\"â could not find 'salary' with #SALARY tag, in p.t-16, in description, or in full-time\")\n",
    "                        salaries.append(None)\n",
    "\n",
    "    # DICTIONARY ASSIGNMENT pass data from lists into a dictionary\n",
    "    dict_from_scrape = {'title':titles, 'company':companies, 'location':locations, 'remote':remote, \n",
    "                        'post_date':post_dates, 'num_applicants':num_applicants, 'contract_type':contract, \n",
    "                        'company_size':size, 'description':desc, 'salary':salaries}\n",
    "\n",
    "    # DATAFRAME ASSIGNMENT\n",
    "    df_from_scrape = pd.DataFrame(dict_from_scrape)\n",
    "\n",
    "    os.system(\"say -v Monica ayam don escreipin\")\n",
    "    return df_from_scrape\n",
    "\n",
    "df = scrape_listing_manually(job_links_cleaned)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827b763f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"About the job\\n          \\n\\n \\n                Do you have a passion for artificial intelligence, machine learning, and data analysis? Do you yearn to have the impact of your work recognized and valued by more than just your development team? Do you constantly wonder what you could build if only you had access to world-class data sets and computing resources?\\n\\nIf yes, we have just the role for you.\\n\\nIn Deloitte's Audit and Assurance business, we make businesses and markets better. An audit is more than an obligation; it is an opportunity to see further and deeper into businesses. In our role as independent auditors, we enhance trust in the companies we audit, helping a multitrillion dollar capital markets system function with greater confidence. As we aspire to the very highest standards of audit quality, we deliver deeper insights that can help clients become more effective organizations.\\n\\nDeloitte's Audit and Assurance business embraces the promise of artificial intelligence and machine learning, with various forms of AI and ML embedded in the audit technology solutions currently used by our 10,000+ practitioners today. Within our dedicated Data Science organization, a subgroup of our award-winning Audit Transformation group, we continue that journey enabling the next generation of AI-enabled solutions that usher in a future that completely transforms the way our practitioners perform their audit work, and the insights we can provide to our clients.\\n\\nYou will be joining a growing team of talented professionals in a fast-paced yet collaborative startup-like environment dedicated to realizing the Deloitte Audit and Assurance vision of an AI-enabled audit. You will be leveraging the most advanced technologies in machine learning, natural language processing, time-series modeling, and reinforcement learning to lead our business into the future.\\n\\nAs a Data Science Manager, you will lead the technical and technological components of our Data Science workstreams and AI and ML solutions. You will work hand-in-hand with subject matter experts to ensure the inputs and outputs suit the intended user experience and audit workflow. You will manage Data Scientists, Junior Data Scientists, and developers to complete the Data Science objectives that are required to fully develop and deploy a production-level ML solution. This includes performing or guiding research in an independent fashion, and interacting closely with key stakeholders with varying levels of machine learning experience.\\n\\nSpecifically, you will be expected to:Have a profound understanding of the state of the art of a multitude of fields in artificial intelligence, including but not limited to NLP, probabilistic graphical models, time-series analysis, and weak supervised learning among othersLead the application of rigorous data science within your workstream, managing and supervising junior resources to do so; particularly:Perform or lead the performance of exploratory data analysis to understand relationships and opportunities to influence outcomes, while being able to quickly iterate common feature transformation and model types to find the best predictive modelsDevelop and document proofs of concept to verify your ideas, including counterfactual explanations for interpretabilityClose the loop to make sure that the proposed solution is performing as it should and is correctly understoodCollaborate with subject matter experts to obtain a deep understanding of the underlying business problem, and to define and refine the corresponding technical solutionCo-lead the planning and direction of a project with subject matter experts, and effectively prioritize goals and objectivesIdentify opportunities to apply the latest advancements in machine learning and artificial intelligence to build, test, and validate predictive modelsMake impactful contributions to internal discussions on emerging machine learning methodologiesInfluence machine learning strategy for current and prospective workstreamsActively mentor Data Scientists and Junior Data Scientists on good software practicesDevelop and embed automated processes for predictive model validation, deployment, and implementationArchitect ML pipelines and actively contribute high-quality, production-ready code (readable, well-tested, with well-designed APIs)\\nQualifications\\n\\nRequired:\\nUndergraduate degree in a quantitative field (computer science, engineering, mathematics, physics, machine learning, statistics)6+ years of industry experience leading the design, development, and deployment of machine learning modelsExperience being the technical lead for multiple project teams simultaneouslyPrevious experience mentoring, training, and developing junior members of the team; experience in employee performance reviewsExpert understanding of Python and other common languagesDeep understanding of machine learning model development life cyclesExtensive experience using common machine learning and deep learning libraries and techniques, including TensorFlow, PyTorch, and big data platformsExtensive experience with cloud-based ecosystems (Azure, GCP, AWS)Experience in an Agile working environment and related project management tools (Jira, Azure DevOps, etc.)Demonstrated ability to write high-quality, production-ready code (readable, well-tested, well-documented, with well-designed APIs)Demonstrated ability to develop novel machine learning methods that go beyond putting together existing open-source code, and to apply problem-solving skills to complex issuesExperience with version control practices and tools (Git, etc.)Fluency in both structured and unstructured data (SQL, NOSQL)Solid understanding of Docker, Jenkins, Kubernetes, and other DevOps toolsExcellent written and verbal communication skillsAbility to travel 30%, on average, based on the work you do and the clients and industries/sectors you serveLimited immigration sponsorship may be available\\nPreferred:\\nPhD in a quantitative field (computer science, engineering, mathematics, physics, machine learning, statistics)10+ years of industry experience leading the design, development, and deployment of machine learning modelsPrior scientific publication history. Outstanding academic track record as evidenced by top tier publications.Strong competency for additional coding languages (R, etc.)Strong project management and delivery experience, including budget oversight and staffing of project teams including time managementExtensive experience with Microsoft Azure, including certification in machine learningExperience with machine learning pipelines (Azure ML)Experience with ML Ops and related governance processes, particularly within a regulated industryStrong presentation skills using Microsoft Office suite (Visio, PowerPoint, etc.)Understanding of the capital markets, and the role public accounting firms\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG beautiful soup test\n",
    "\n",
    "with open('data_page.html', 'r') as f:\n",
    "\n",
    "    contents = f.read()\n",
    "\n",
    "    soup_local = BeautifulSoup(contents, 'lxml')\n",
    "\n",
    "    #print(soup_local)\n",
    "\n",
    "soup_local.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8edccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping job 0 of 1085.\n",
      "remote could not be found in header\n",
      "could not find 'number of applicants' in applicant count or bullet\n",
      "could not find \"contract type\" in job insights.span\n",
      "could not find description (probably shouldn't apply!)\n",
      "\n",
      "Scraping job 1 of 1085.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# SCRAPE each page for dataframe info\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_pages_for_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_links_cleaned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m driver\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# DISPLAY our new dataframe (uncleaned)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mscrape_pages_for_df\u001b[0;34m(links)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_pages_for_df\u001b[39m(links):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\" SCRAPE each link in the newly cleaned list \"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mscrape_listing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_links_cleaned\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;66;03m# FIX: maybe add: .reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# DEBUG: use this method to get a partial dataframe when you cut the code off early\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#df = pd.DataFrame(scrape_listing(job_links_cleaned[0:50]))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#for i in len(job_links_cleaned)//50:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#    df = pd.concat([df, scrape_listing(job_links_cleaned[i*50:i*50+50])], axis=0).reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mscrape_listing\u001b[0;34m(links)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# GO TO PAGE Navigate to page\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print('\\n\\n\\nkey:', key, '\\nvalue:', value[0][0]) # DEBUG TEXT\u001b[39;00m\n\u001b[1;32m     14\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://linkedin.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# SEE FULL PAGE click 'see more' and scroll down\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#see_more() # FIX\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# BEAUTFUL SOUP EXTRACTION convert page text to beautiful soup\u001b[39;00m\n\u001b[1;32m     21\u001b[0m src \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def scrape_pages_for_df(links):\n",
    "    \"\"\" SCRAPE each link in the newly cleaned list \"\"\"\n",
    "    #df = pd.DataFrame(scrape_listing(job_links_cleaned))# FIX: maybe add: .reset_index(drop=True)\n",
    "    \n",
    "    # DEBUG: use this method to get a partial dataframe when you cut the code off early\n",
    "    df = pd.DataFrame(scrape_listing(job_links_cleaned[0:50]))\n",
    "    for i in len(job_links_cleaned)//50:\n",
    "        df = pd.concat([df, scrape_listing(job_links_cleaned[i*50:i*50+50])], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# SCRAPE each page for dataframe info\n",
    "df = scrape_pages_for_df(job_links_cleaned)\n",
    "driver.close()\n",
    "\n",
    "# DISPLAY our new dataframe (uncleaned)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47071d8",
   "metadata": {},
   "source": [
    "## Scrape Glassdoor using the Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search glassdoor for all job titles\n",
    "# save to glass_df: job title, salary range (or salary avg? both?)\n",
    "# add glassdoor_salary column to df\n",
    "# fill glassdoor_salary based on role name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdba8e",
   "metadata": {},
   "source": [
    "## Export Data to CSV for Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a52820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/output/linkedin_jobs_uncleaned.csv', index = False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
