{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e3c1ba",
   "metadata": {},
   "source": [
    "# LinkedIn Market Analysis\n",
    "\n",
    "\n",
    "### Process:\n",
    "1. Scrape data from Linkedin and Glassdoor, using Selenium.\n",
    "2. EDA, cleaning, and export to CSV.\n",
    "3. Compare to previous years' data (2 and 4 years ago) using tableau.\n",
    "\n",
    "### Additional Datasets:\n",
    "- 2018: https://www.kaggle.com/datasets/discdiver/data-scientist-general-skills-2018-revised (skills specific)\n",
    "- 2020: https://www.kaggle.com/datasets/andrewmvd/data-analyst-jobs (jobs, salary, and location)\n",
    "- 2022: [this notebook]\n",
    "\n",
    "### Reference Notebooks:\n",
    "- https://www.kaggle.com/code/gawainlai/us-data-science-job-salary-regression-w-visuals (beyond my skill level)\n",
    "- https://www.kaggle.com/code/discdiver/the-most-in-demand-skills-for-data-scientists (top skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafd57a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# web scraping imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# database imports\n",
    "import re as re\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# import function to save different files to csv (ie, job links)\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "# import for description scrape\n",
    "import urllib.request\n",
    "\n",
    "# import and load file to login to LinkedIn\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3331f3",
   "metadata": {},
   "source": [
    "## Scrape LinkedIn for Job Links Using the Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a23b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the LinkedIn scrape\n",
    "\n",
    "# Options\n",
    "options = webdriver.ChromeOptions() # init for chrome\n",
    "options.add_argument('--incognito') # runs chrome in a 'clean slate' window\n",
    "#options.add_argument('--headless') # runs chromedriver in the background, without opening a window\n",
    "\n",
    "# Initialize the selenium driver\n",
    "driver = webdriver.Chrome(options = options, executable_path='./chromedriver')\n",
    "login_url = \"https://www.linkedin.com/uas/login\"\n",
    "\n",
    "# Start the page\n",
    "driver.get(login_url)\n",
    "time.sleep(3)\n",
    "\n",
    "# Target the login elements\n",
    "email = driver.find_element(\"id\", \"username\")\n",
    "password = driver.find_element(\"id\", \"password\")\n",
    "\n",
    "# Load env variables\n",
    "my_email = os.getenv(\"linkedin_username\")\n",
    "my_password = os.getenv(\"linkedin_password\")\n",
    "\n",
    "# Input in the form\n",
    "email.send_keys(my_email)\n",
    "password.send_keys(my_password)\n",
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(data_role, location):\n",
    "    \"\"\" Scrape 40 pages of a LinkedIn job search for job links, using the given data role as the search term \"\"\"\n",
    "    \n",
    "    # SCRAPE 40 PAGES\n",
    "    for i in range(10): # FIX: change back to 40 for final analysis\n",
    "        print(f'Scraping {i+1} of 40 pages for {data_role} in {location}.')\n",
    "        \n",
    "        # navigate to the correct page\n",
    "        scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&location={location}&refresh=true&start={i*25}\"\n",
    "        # TEST: https://www.linkedin.com/jobs/search/?&keywords=data%20analyst&location=Los%20Angeles%2C%20California%2C%20United%20States&refresh=true&start=1\n",
    "        if i == 0:\n",
    "            scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&location={location}&refresh=true&start={1}\"\n",
    "        driver.get(scrape_url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # convert page text to beautiful soup\n",
    "        src = driver.page_source\n",
    "        soup_for_page = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        # create a list of jobs on the current page, to iterate through after each scrape\n",
    "        job_links = []\n",
    "        jobs_on_page = soup_for_page.find_all(\"a\", attrs={\"class\":\"disabled ember-view job-card-container__link job-card-list__title\"})\n",
    "        for k in jobs_on_page: # length of jobs varies by page\n",
    "            job_links.append(k[\"href\"])\n",
    "        print(f'Job links collected from page {i+1}:', len(job_links)) # DEBUG\n",
    "        #print(f'job links from page {i+1}:',job_links) # DEBUG\n",
    "        \n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bde0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_2d_list(list_):\n",
    "    \"\"\" Flatten a 2d list of lists into a 1d list \"\"\"\n",
    "    list_flattened = [a for y in list_ for a in y]\n",
    "    return list(set(list_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20270abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LISTS FOR SCRAPING for job links\n",
    "\n",
    "# 5 titles taken from market analysis, used to capture all links to scrape with matching search term.\n",
    "data_roles = ['data analyst','data scientist','data engineer','data architect','data manager']\n",
    "# removed from final scrape to reduce noise and risk of account ban: 'finance analyst','data warehouse analyst','data manager','data marketing analyst'\n",
    "\n",
    "# 32 locations chosen from top tech cities across US (excluding search results yielding the same listings on LinkedIn)\n",
    "locations = ['San Francisco, California, United States','Los Angeles, California, United States','San Jose, California, United States',\n",
    "             'San Diego, California, United States','Portland, Oregon, United States','Seattle, Washington, United States',\n",
    "             'Denver, Colorado, United States', 'Colorado Springs, Colorado, United States','Indianapolis, Indiana, United States',\n",
    "             'New York, New York, United States','Secaucus, New Jersey', 'Boston, Massachusetts, United States', \n",
    "             'Baltimore, Maryland, United States','Chicago, Illinois, United States','Philadelphia, Pennsylvania, United States'\n",
    "             'Phoenix, Arizona, United States','Salt Lake City, Utah, United States','Minneapolis, Minnesota, United States',\n",
    "             'Detroit, Michigan, United States','Columbus, Ohio, United States','Kansas City, Missouri, United States',\n",
    "             'Austin, Texas, United States','Dallas, Texas, United States','Houston, Texas, United States', \n",
    "             'Atlanta, Georgia, United States','Jackson, Mississippi, United States','Washington, District of Columbia, United States',\n",
    "             'Charlotte, North Carolina, United States','Raleigh, North Carolina, United States',\n",
    "             'Jacksonville, Florida, United States','Miami, Florida, United States','Tampa, Florida, United States']\n",
    "\n",
    "\"\"\"\n",
    "HOW MUCH DATA IS ENOUGH ?\n",
    "35 locations * 9 titles * 2 pages = 630 pages. At the full 40 pages, the real total of my scrape will be \n",
    "12,600 pages. Assuming I don't get banned for scraping 10,000 pages, let alone 100 pages, I will still need \n",
    "to scrape the links that come from them. That's 630 pages * 7 links = 4,410 links for the sample and \n",
    "12,600 * 25 = 315,000 links for the real scrape. Of course, most jobs will be duplicates, but that doesn't \n",
    "change that I will have to wait a long time for data and I may get banned several times before the scrape is\n",
    "complete. In reality, it may be safer to limit my searches to fewer titles, locations, pages and links.\n",
    "\n",
    "The new scrape of 5 roles * 32 cities * 40 pages = 6,400 (or 320 for the 2-page sample) is more reasonable.\n",
    "\n",
    "state_locations = ['Washington', 'California', 'Colorado', 'Texas', 'Illinois', 'Florida', 'Atlanta', 'New York']\n",
    "\n",
    "global_locations = [Barcelona, Madrid, Berlin, Munich, Amsterdam, London, Dublin, Stockholm, Copenhagen, Oslo,\n",
    "             Luxembourg, Eindhoven, Manchester, Belfast, Bristol, Paris, Budapest, Bucharest, Warsaw, Prague, \n",
    "             Lisbon, Rome, Zurich, vancouver, ontario, montreal, toronto, \n",
    "             Melbourne, Moscow, Seoule, Jakarta, Kyiv, tokyo, rejkjavik,\n",
    "             argentina, mexico city, lima, rio, buenos aires, sao paolo, panama,] \n",
    "\"\"\"\n",
    "\n",
    "job_links = [] # init list to capture all job links\n",
    "\n",
    "# SCRAPE search LinkedIn for each role title and location given above and return a list of up to 1,000 jobs\n",
    "for title in data_roles:\n",
    "    for location in locations:\n",
    "        print(f'Searching for {title} jobs in {location}...')\n",
    "        job_links.append(scrape_links(title.replace(' ','%20'), location.replace(',','%2C').replace(' ','%20')))\n",
    "#print(job_links) # DEBUG\n",
    "\n",
    "# DUPLICATES remove dupliate links and flatten 2d array before scraping\n",
    "job_links_cleaned = flatten_2d_list(job_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb7a97",
   "metadata": {},
   "source": [
    "## Export (and Import) Job Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export links to csv for future use\n",
    "with open(f'output/job_links_{datetime.date.today()}.csv', 'w', newline='') as job_links:\n",
    "    csv_out = csv.writer(job_links)\n",
    "    csv_out.writerows([job_links_cleaned[index]] for index in range(0, len(job_links_cleaned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import job links from csv\n",
    "with open('output/job_links_sample.csv', newline='') as f: # change for desired search date\n",
    "    reader = csv.reader(f)\n",
    "    job_links_imported = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = flatten_2d_list(job_links_imported)\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4b57c",
   "metadata": {},
   "source": [
    "## Scrape All Job Links Using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPE FOR ALL DATA SANS DESCRIPTION ?\n",
    "\n",
    "def scrape_listing(links):\n",
    "    \"\"\" Returns all scraped data for each job listing from the links passed into the function \"\"\"\n",
    "\n",
    "    # VARIABLE ASSIGNMENT create lists to store all scraped data (10 criteria)\n",
    "    titles, companies, locations, remote, post_dates, num_applicants, contract, size, desc, salaries = [], \\\n",
    "        [], [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # SCRAPE ALL LINKS scrape all jobs on the current page using passed in links\n",
    "    for idx, link in enumerate(links[:1000]):\n",
    "        print(f'\\nScraping job {idx} of {len(links)}.') # DEBUG TEXT\n",
    "        \n",
    "        # GO TO PAGE Navigate to page\n",
    "        #print('\\n\\n\\nkey:', key, '\\nvalue:', value[0][0]) # DEBUG TEXT\n",
    "        driver.get(f'https://linkedin.com{link}')\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # SEE FULL PAGE click 'see more' and scroll down\n",
    "        #see_more() # FIX\n",
    "        \n",
    "        # BEAUTFUL SOUP EXTRACTION convert page text to beautiful soup\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        # DATA COLLECTION return data of results on current selected sub-page to new lists\n",
    "        \n",
    "        # TITLE\n",
    "        try:\n",
    "            titles.append(soup.select('h1.t-24.t-bold.jobs-unified-top-card__job-title')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print('title could not be found')\n",
    "            titles.append(None)\n",
    "         \n",
    "        # COMPANY\n",
    "        try:\n",
    "            companies.append(soup.select('span.jobs-unified-top-card__company-name')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print('company could not be found')\n",
    "            companies.append(None)\n",
    "            \n",
    "        # LOCATION\n",
    "        try:\n",
    "            locations.append(soup.select('span.jobs-unified-top-card__bullet')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print('location could not be found')\n",
    "            locations.append(None)\n",
    "        \n",
    "        # REMOTE POSITION\n",
    "        try: # check in header\n",
    "            remote.append(soup.select('span.jobs-unified-top-card__workplace-type')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try: # check the description for remote term.\n",
    "                desc_temp = soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip()\n",
    "                print('remote could not be found in header')\n",
    "                if 'remote' in desc_temp:\n",
    "                    remote.append('remote?')\n",
    "                elif 'hybrid' in desc:\n",
    "                    remote.append('hybrid?')\n",
    "                else:\n",
    "                    remote.append(None)\n",
    "            except:\n",
    "                try: # check in title\n",
    "                    title_temp = soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip()\n",
    "                    if 'remote' in title_temp:\n",
    "                        remote.append('remote?')\n",
    "                    else:\n",
    "                        remote.append(None)\n",
    "                    print('remote position could not be found in header or description')\n",
    "                except:\n",
    "                    print('remote position could not be found in header , description, or title')\n",
    "                    remote.append(None)\n",
    "            \n",
    "        # POST DATE\n",
    "        try:\n",
    "            post_dates.append(soup.select('span.jobs-unified-top-card__posted-date')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'posted date'\")\n",
    "            post_dates.append(None)\n",
    "        \n",
    "        # NUMBER of APPLICANTS\n",
    "        try:\n",
    "            num_applicants.append(soup.select('span.jobs-unified-top-card__applicant-count')[2].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                num_applicants.append(soup.select('span.jobs-unified-top-card__bullet')[1].get_text().replace('\\n','').strip())\n",
    "                print(\"could not find 'number of applicants' in applicant count\")\n",
    "            except:\n",
    "                print(\"could not find 'number of applicants' in applicant count or bullet\")\n",
    "                num_applicants.append(None)\n",
    "        \n",
    "        # FULL TIME\n",
    "        try:\n",
    "            contract.append(soup.select('li.jobs-unified-top-card__job-insight.span').get_text()\n",
    "                .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "        except:\n",
    "            try:\n",
    "                contract.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                    .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "                print('could not find \"contract type\" in job insights.span')\n",
    "            except:\n",
    "                print(\"could not find 'contract type' in job insights.span or job insights\")\n",
    "                contract.append(None)\n",
    "        \n",
    "        # COMPANY SIZE\n",
    "        try:\n",
    "            size.append(soup.select('li.jobs-unified-top-card__job-insight')[1].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'company size'\")\n",
    "            size.append(None)\n",
    "        \n",
    "        # DESCRIPTION\n",
    "        try:\n",
    "            desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip())\n",
    "        except:\n",
    "            \n",
    "            print(\"could not find description (probably shouldn't apply!)\")\n",
    "            \n",
    "        # SALARY\n",
    "        try:\n",
    "            salaries.append(soup.select('a.app-aware-link')[6].get_text().replace('\\n','').strip())\n",
    "            if '$' not in salaries[-1]:\n",
    "                salaries.pop()\n",
    "                salaries.append(None)\n",
    "        except:\n",
    "            try:\n",
    "                salaries.append(soup.select('p.t-16')[0].get_text().replace('\\n','').strip())\n",
    "                print(\"could not find 'salary' with #SALARY tag\")\n",
    "            except:\n",
    "                try:\n",
    "                    salaries.append(re.find('($.)', desc).replace('\\n','').strip())\n",
    "                    print(\"could not find 'salary' with #SALARY tag or in p.t-16\")\n",
    "                except:\n",
    "                    try: \n",
    "                        salaries.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                            .replace('\\n','').strip().rsplit(' ', 1)[0].rstrip(' ·'))\n",
    "                        print(\"could not find 'salary' with #SALARY tag, in p.t-16, or in description\")\n",
    "                    except:\n",
    "                        print(\"could not find 'salary' with #SALARY tag, in p.t-16, in description, or in full-time\")\n",
    "                        salaries.append(None)\n",
    "\n",
    "    # DICTIONARY ASSIGNMENT pass data from lists into a dictionary\n",
    "    dict_from_scrape = {'title':titles, 'company':companies, 'location':locations, 'remote':remote, \n",
    "                        'post_date':post_dates, 'num_applicants':num_applicants, 'contract_type':contract, \n",
    "                        'company_size':size, 'description':desc, 'salary':salaries}\n",
    "\n",
    "    # DATAFRAME ASSIGNMENT\n",
    "    df_from_scrape = pd.DataFrame(dict_from_scrape)\n",
    "    \n",
    "    os.system(\"say -v Monica ayam don escreipin\")\n",
    "    return df_from_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97aa288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPE FOR DATA FROM DESCRIPTION ?\n",
    "\n",
    "def scrape_listing_with_soup(links):\n",
    "    \"\"\" Returns all scraped data for each job listing from the links passed into the function \"\"\"\n",
    "\n",
    "    # VARIABLE ASSIGNMENT create lists to store all scraped data (10 criteria)\n",
    "    titles, companies, locations, remote, post_dates, num_applicants, contract, size, desc, salaries = [], \\\n",
    "        [], [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # SCRAPE ALL LINKS scrape all jobs on the current page using passed in links\n",
    "    for idx, link in enumerate(links[:10]):\n",
    "        print(f'\\nScraping job {idx+1} of {len(links)}.\\n') # DEBUG TEXT\n",
    "        print(f'https://linkedin.com{link}\\n') # DEBUG TEXT\n",
    "        #opener = urllib.request.FancyURLopener({})\n",
    "        opener = urllib.request.urlopen(f'https://linkedin.com{link}')\n",
    "        #with opener.open(f'https://linkedin.com{link}') as f: \n",
    "        #    f.read().decode('utf-8')\n",
    "            #content = f.read()\n",
    "        #with open('data_page.html', 'r') as f:\n",
    "            #contents = f.read()\n",
    "        soup = BeautifulSoup(opener, 'lxml')\n",
    "        #print('empty soup', soup)\n",
    "\n",
    "        # DATA COLLECTION return data of results on current selected sub-page to new lists\n",
    "        \n",
    "        # TITLE\n",
    "        try:\n",
    "            titles.append(soup.select('h1.t-24.t-bold.jobs-unified-top-card__job-title')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                titles.append(soup.select('h1.top-card-layout__title.font-sans.text-lg.papabear:text-xl.font-bold.leading-open.text-color-text.mb-0.topcard__title').get_text().strip())\n",
    "            except:\n",
    "                print('title could not be found')\n",
    "                titles.append(None)\n",
    "\n",
    "        # COMPANY\n",
    "        try:\n",
    "            companies.append(soup.select('span.jobs-unified-top-card__company-name')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                companies.append(soup.select('a.topcard__org-name-link.topcard__flavor--black-link').get_text().strip())\n",
    "            except:    \n",
    "                print('company could not be found')\n",
    "                companies.append(None)\n",
    "\n",
    "        # LOCATION\n",
    "        try:\n",
    "            locations.append(soup.select('span.jobs-unified-top-card__bullet')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                locations.append(soup.select('span.topcard__flavor.topcard__flavor--bullet').get_text().strip())\n",
    "            except: \n",
    "                print('location could not be found')\n",
    "                locations.append(None)\n",
    "\n",
    "        # REMOTE POSITION\n",
    "        try: # check in header\n",
    "            remote.append(soup.select('span.jobs-unified-top-card__workplace-type')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try: # check the description for remote term.\n",
    "                desc_temp = soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip()\n",
    "                print('remote could not be found in header')\n",
    "                if 'remote' in desc_temp:\n",
    "                    remote.append('remote?')\n",
    "                elif 'hybrid' in desc:\n",
    "                    remote.append('hybrid?')\n",
    "                else:\n",
    "                    remote.append(None)\n",
    "            except:\n",
    "                try: # check in title\n",
    "                    title_temp = soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip()\n",
    "                    if 'remote' in title_temp:\n",
    "                        remote.append('remote?')\n",
    "                    else:\n",
    "                        remote.append(None)\n",
    "                    print('remote position could not be found in header or description')\n",
    "                except:\n",
    "                    print('remote position could not be found in header , description, or title')\n",
    "                    remote.append(None)\n",
    "\n",
    "        # POST DATE\n",
    "        try:\n",
    "            post_dates.append(soup.select('span.jobs-unified-top-card__posted-date')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"❌ could not find 'posted date'\")\n",
    "            post_dates.append(None)\n",
    "\n",
    "        # NUMBER of APPLICANTS\n",
    "        try:\n",
    "            num_applicants.append(soup.select('span.jobs-unified-top-card__applicant-count')[2].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            try:\n",
    "                num_applicants.append(soup.select('span.jobs-unified-top-card__bullet')[1].get_text().replace('\\n','').strip())\n",
    "                print(\"could not find 'number of applicants' in applicant count\")\n",
    "            except:\n",
    "                try:\n",
    "                    num_applicants.append(soup.select('num_applicants__caption').get_text().replace('\\n','').strip())\n",
    "                    print(\"could not find 'number of applicants' in applicant count or bullet\")\n",
    "                except:\n",
    "                    print(\"❌ could not find 'number of applicants' in applicant count, bullet, or caption\")\n",
    "                    num_applicants.append(None)\n",
    "\n",
    "        # FULL TIME\n",
    "        try:\n",
    "            contract.append(soup.select('li.jobs-unified-top-card__job-insight.span').get_text()\n",
    "                .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "        except:\n",
    "            try:\n",
    "                contract.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                    .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "                print('could not find \"contract type\" in job insights.span')\n",
    "            except:\n",
    "                print(\"❌ could not find 'contract type' in job insights.span or job insights\")\n",
    "                contract.append(None)\n",
    "\n",
    "        # COMPANY SIZE\n",
    "        try:\n",
    "            size.append(soup.select('li.jobs-unified-top-card__job-insight')[1].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"❌ could not find 'company size'\")\n",
    "            size.append(None)\n",
    "\n",
    "        # DESCRIPTION\n",
    "        try:\n",
    "            desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip())\n",
    "        except:\n",
    "            try:\n",
    "                desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch.span').get_text().strip())\n",
    "            except:\n",
    "                try:\n",
    "                    desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch').get_text().strip())\n",
    "                except:\n",
    "                    try:\n",
    "                        desc.append(soup.select('div.job-details.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip())\n",
    "                    except:\n",
    "                        try:\n",
    "                            print('text rich\\n', soup.select('div.description__text.description__text--rich')[0].get_text().strip())\n",
    "                            desc.append(soup.select('div.description__text.description__text--rich')[0].get_text().strip())\n",
    "                        except:\n",
    "                            try:\n",
    "                                desc.append(soup.select('div.show-more-less-html__markup.show-more-less-html__markup--clamp-after-5')[0])\n",
    "                                print('clamp after 5\\n', soup.select('div.show-more-less-html__markup.show-more-less-html__markup--clamp-after-5')[0])\n",
    "                            except:\n",
    "                                desc.append(None)\n",
    "                                print(\"⚠️ could not find description.\")\n",
    "\n",
    "        # SALARY\n",
    "        try:\n",
    "            salaries.append(soup.select('a.app-aware-link')[6].get_text().replace('\\n','').strip())\n",
    "            if '$' not in salaries[-1]:\n",
    "                salaries.pop()\n",
    "                salaries.append(None)\n",
    "        except:\n",
    "            try:\n",
    "                salaries.append(soup.select('p.t-16')[0].get_text().replace('\\n','').strip())\n",
    "                print(\"could not find 'salary' with #SALARY tag\")\n",
    "            except:\n",
    "                try:\n",
    "                    salaries.append(re.find('($.)', desc).replace('\\n','').strip())\n",
    "                    print(\"could not find 'salary' with #SALARY tag or in p.t-16\")\n",
    "                except:\n",
    "                    try: \n",
    "                        salaries.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                            .replace('\\n','').strip().rsplit(' ', 1)[0].rstrip(' ·'))\n",
    "                        print(\"could not find 'salary' with #SALARY tag, in p.t-16, or in description\")\n",
    "                    except:\n",
    "                        print(\"❌ could not find 'salary' with #SALARY tag, in p.t-16, in description, or in full-time\")\n",
    "                        salaries.append(None)\n",
    "\n",
    "    # DICTIONARY ASSIGNMENT pass data from lists into a dictionary\n",
    "    dict_from_scrape = {'title':titles, 'company':companies, 'location':locations, 'remote':remote, \n",
    "                        'post_date':post_dates, 'num_applicants':num_applicants, 'contract_type':contract, \n",
    "                        'company_size':size, 'description':desc, 'salary':salaries}\n",
    "\n",
    "    # DATAFRAME ASSIGNMENT\n",
    "    df_from_scrape = pd.DataFrame(dict_from_scrape)\n",
    "\n",
    "    os.system(\"say -v Monica ayam don escreipin\")\n",
    "    return df_from_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d67d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scrape_listing(links))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e773811",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(scrape_listing_with_soup(links))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4574be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scrape_listing(links)) # FIX: maybe add: .reset_index(drop=True)\n",
    "df.insert(0, 'descriptions', scrape_listing_with_soup(links))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdba8e",
   "metadata": {},
   "source": [
    "## Export Data to CSV for Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a52820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'output/linkedin_jobs_uncleaned_full_{datetime.date.today()}.csv', index = False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
