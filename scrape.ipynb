{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e3c1ba",
   "metadata": {},
   "source": [
    "# LinkedIn Market Analysis\n",
    "\n",
    "\n",
    "### Process:\n",
    "1. Scrape data from Linkedin and Glassdoor, using Selenium.\n",
    "2. EDA, cleaning, and export to CSV.\n",
    "3. Compare to previous years' data (2 and 4 years ago) using tableau.\n",
    "\n",
    "### Additional Datasets:\n",
    "- 2018: \n",
    "- 2020: \n",
    "- 2022: [my submission to Kaggle]\n",
    "\n",
    "### Reference Notebooks:\n",
    "- https://www.kaggle.com/code/gawainlai/us-data-science-job-salary-regression-w-visuals (beyond my skill level)\n",
    "- https://www.kaggle.com/code/discdiver/the-most-in-demand-skills-for-data-scientists (top skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafd57a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ca6211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# web scraping imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# database imports\n",
    "import re as re\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "\n",
    "# import and load file to login to LinkedIn\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3331f3",
   "metadata": {},
   "source": [
    "## Scrape LinkedIn Using the Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a23b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/4xsyrpbs305132t_87mnd3600000gn/T/ipykernel_78282/3666220930.py:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(options = options, executable_path='./chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# initialize the LinkedIn scrape\n",
    "\n",
    "# Options\n",
    "options = webdriver.ChromeOptions() # init for chrome\n",
    "options.add_argument('--incognito') # runs chrome in a 'clean slate' window\n",
    "#options.add_argument('--headless') # runs chromedriver in the background, without opening a window\n",
    "\n",
    "# Initialize the selenium driver\n",
    "driver = webdriver.Chrome(options = options, executable_path='./chromedriver')\n",
    "login_url = \"https://www.linkedin.com/uas/login\"\n",
    "\n",
    "# Start the page\n",
    "driver.get(login_url)\n",
    "time.sleep(3)\n",
    "\n",
    "# Target the login elements\n",
    "email = driver.find_element(\"id\", \"username\")\n",
    "password = driver.find_element(\"id\", \"password\")\n",
    "\n",
    "# Load env variables\n",
    "my_email = os.getenv(\"linkedin_username\")\n",
    "my_password = os.getenv(\"linkedin_password\")\n",
    "\n",
    "# Input in the form\n",
    "email.send_keys(my_email)\n",
    "password.send_keys(my_password)\n",
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6489186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(data_role):\n",
    "    \"\"\" Scrape 40 pages of a LinkedIn job search for job links, using the given data role as the search term \"\"\"\n",
    "    \n",
    "    # SCRAPE 40 PAGES\n",
    "    for i in range(2): # FIX: set 2 to 40 for final product\n",
    "        print(f'running search for {i+1} of 40 pages for {data_role}.')\n",
    "        \n",
    "        # navigate to the correct page\n",
    "        scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&refresh=true&start={i*25}\"\n",
    "        if i == 0:\n",
    "            scrape_url = f\"https://www.linkedin.com/jobs/search/?&keywords={data_role}&refresh=true&start={1}\"\n",
    "        driver.get(scrape_url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # SCROLL DOWN THE JOB LIST\n",
    "        #driver.execute_script(\"window.scrollTo(0, document.body.div.jobs-search-results-list.scrollHeight);\")\n",
    "        #left_rail_bottom = driver.find_element(By.CLASS_NAME, \"global-footer-compact__content.t-12.t-normal.text-align-center.clear-both.compactfooter-copyright\")\n",
    "        #left_rail_bottom = driver.find_element(By.CSS_SELECTOR, \"li.global-footer-compact__item\")\n",
    "        left_rail_bottom = driver.find_element(By.ID, \"compactfooter-about\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", left_rail_bottom)\n",
    "        \n",
    "        # scroll down for each job element\n",
    "        #driver.execute_script(\"arguments[0].scrollIntoView();\", job)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # convert page text to beautiful soup\n",
    "        src = driver.page_source\n",
    "        soup_for_page = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        # create a list of jobs on the current page, to iterate through after each scrape\n",
    "        job_links = []\n",
    "        jobs_on_page = soup_for_page.find_all(\"a\", attrs={\"class\":\"disabled ember-view job-card-container__link job-card-list__title\"})\n",
    "        for k in jobs_on_page: # length of jobs varies by page\n",
    "            job_links.append(k[\"href\"])\n",
    "        print(f'total job links from page {i+1}:', len(job_links)) # DEBUG\n",
    "        #print(f'job links from page {i+1}:',job_links) # DEBUG\n",
    "        \n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088f1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_more():\n",
    "    \"\"\" Load an entire LinkedIn job post page \"\"\"\n",
    "    \n",
    "    # Click on the 'see more' button on a LinkedIn job post (there are several window types)\n",
    "    try:\n",
    "        driver.find_element_by_class_name(\"artdeco-card__actions\").click()\n",
    "        print('find_element_by_class_name(\"artdeco-card__actions\") SUCCESS')\n",
    "    except:\n",
    "        try:\n",
    "            driver.find_element(By.Class, \"show-more-less-html__button show-more-less-html__button--more\").click()\n",
    "            print('driver.find_element(By.Class, \"show-more-less-html__button ... SUCCESS')\n",
    "        except:\n",
    "            print('Find another solution to SEE MORE on this page')\n",
    "                            \n",
    "    # Scroll down the entire page for easier data collection\n",
    "    driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2206d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listing(links):\n",
    "    \"\"\" Returns all scraped data for each job listing from the links passed into the function \"\"\"\n",
    "\n",
    "    # VARIABLE ASSIGNMENT create lists to store all scraped data (10 criteria)\n",
    "    titles, companies, locations, remote, post_dates, num_applicants, full_time, size, desc, salaries = [], \\\n",
    "        [], [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # SCRAPE ALL LINKS scrape all jobs on the current page using passed in links\n",
    "    #for key, value in job_links.items():\n",
    "    for link in links:\n",
    "    \n",
    "        # GO TO PAGE Navigate to page\n",
    "        #print('\\n\\n\\nkey:', key, '\\nvalue:', value[0][0]) # DEBUG TEXT\n",
    "        driver.get(f'https://linkedin.com{link}')\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # SEE FULL PAGE click 'see more' and scroll down\n",
    "        see_more()\n",
    "        \n",
    "        # BEAUTFUL SOUP EXTRACTION convert page text to beautiful soup\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        # DATA COLLECTION return data of results on current selected sub-page to new lists\n",
    "        titles.append(soup.select('h1.t-24.t-bold.jobs-unified-top-card__job-title')[0].get_text().replace('\\n','').strip())\n",
    "        companies.append(soup.select('span.jobs-unified-top-card__company-name')[0].get_text().replace('\\n','').strip())\n",
    "        locations.append(soup.select('span.jobs-unified-top-card__bullet')[0].get_text().replace('\\n','').strip())\n",
    "        remote.append(soup.select('span.jobs-unified-top-card__workplace-type')[0].get_text().replace('\\n','').strip())\n",
    "        \n",
    "        # POST DATE\n",
    "        try:\n",
    "            post_dates.append(soup.select('span.jobs-unified-top-card__posted-date')[0].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'posted date'\")\n",
    "            post_dates.append(None)\n",
    "        \n",
    "        # NUMBER of APPLICANTS\n",
    "        try:\n",
    "            num_applicants.append(soup.select('span.jobs-unified-top-card__applicant-count')[2].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'number of applicants' in applicant count\")\n",
    "            try:\n",
    "                num_applicants.append(soup.select('span.jobs-unified-top-card__bullet')[1].get_text().replace('\\n','').strip())\n",
    "            except:\n",
    "                print(\"could not find 'number of applicants' in bullet\")\n",
    "                num_applicants.append(None)\n",
    "        \n",
    "        # FULL TIME\n",
    "        try:\n",
    "            full_time.append(soup.select('li.jobs-unified-top-card__job-insight.span').get_text()\n",
    "                .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "        except:\n",
    "            print('could not find \"full time\" in job insights')\n",
    "            try:\n",
    "                full_time.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                    .replace('\\n','').strip().rsplit(' ', 1)[-1])\n",
    "            except:\n",
    "                print(\"could not find 'full time'\")\n",
    "                full_time.append(None)\n",
    "        \n",
    "        # COMPANY SIZE\n",
    "        try:\n",
    "            size.append(soup.select('li.jobs-unified-top-card__job-insight')[1].get_text().replace('\\n','').strip())\n",
    "        except:\n",
    "            print(\"could not find 'company size'\")\n",
    "            size.append(None)\n",
    "        \n",
    "        # DESCRIPTION\n",
    "        desc.append(soup.select('div.jobs-box__html-content.jobs-description-content__text.t-14.t-normal.jobs-description-content__text--stretch')[0].get_text().strip())\n",
    "\n",
    "        # SALARY\n",
    "        try:\n",
    "            salaries.append(soup.select('a.app-aware-link')[6].get_text().replace('\\n','').strip())\n",
    "            if '$' not in salaries[-1]:\n",
    "                salaries.pop()\n",
    "                salaries.append(None)\n",
    "        except:\n",
    "            print(\"could not find 'salary' with #SALARY tag\")\n",
    "            try:\n",
    "                salaries.append(soup.select('p.t-16')[0].get_text().replace('\\n','').strip())\n",
    "            except:\n",
    "                print(\"could not find 'salary' on p.t-16\")\n",
    "                try:\n",
    "                    salaries.append(re.find('($.)', desc).replace('\\n','').strip())\n",
    "                except:\n",
    "                    print(\"could not find 'salary' in description\")\n",
    "                    try: \n",
    "                        salaries.append(soup.select('li.jobs-unified-top-card__job-insight')[0].get_text()\n",
    "                            .replace('\\n','').strip().rsplit(' ', 1)[0].rstrip(' ·'))\n",
    "                    except:\n",
    "                        print(\"could not find 'salary' in full-time\")\n",
    "                        salaries.append(None)\n",
    "\n",
    "    # DICTIONARY ASSIGNMENT pass data from lists into a dictionary\n",
    "    dict_from_scrape = {'title':titles, 'company':companies, 'location':locations, 'post_date':post_dates, \n",
    "                        'num_applicants':num_applicants, 'full_time':full_time, 'size':size, \n",
    "                        'description':desc, 'salary':salaries}\n",
    "\n",
    "    # DATAFRAME ASSIGNMENT\n",
    "    df_from_scrape = pd.DataFrame(dict_from_scrape)\n",
    "\n",
    "    return df_from_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70b2de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(list_):\n",
    "    list_flattened = [a for y in list_ for a in y]\n",
    "    return list(set(list_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20270abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running search for 1 of 40 pages for data%20analyst.\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"compactfooter-about\"]\"}\n  (Session info: chrome=104.0.5112.101)\nStacktrace:\n0   chromedriver                        0x000000010631f149 chromedriver + 4469065\n1   chromedriver                        0x00000001062a9233 chromedriver + 3985971\n2   chromedriver                        0x0000000105f3ffe8 chromedriver + 409576\n3   chromedriver                        0x0000000105f761d8 chromedriver + 631256\n4   chromedriver                        0x0000000105f76451 chromedriver + 631889\n5   chromedriver                        0x0000000105fa88f4 chromedriver + 837876\n6   chromedriver                        0x0000000105f93c8d chromedriver + 752781\n7   chromedriver                        0x0000000105fa6611 chromedriver + 828945\n8   chromedriver                        0x0000000105f93b53 chromedriver + 752467\n9   chromedriver                        0x0000000105f69905 chromedriver + 579845\n10  chromedriver                        0x0000000105f6a955 chromedriver + 584021\n11  chromedriver                        0x00000001062f06ad chromedriver + 4277933\n12  chromedriver                        0x00000001062f4b3a chromedriver + 4295482\n13  chromedriver                        0x00000001062f9cdf chromedriver + 4316383\n14  chromedriver                        0x00000001062f5857 chromedriver + 4298839\n15  chromedriver                        0x00000001062ce64f chromedriver + 4138575\n16  chromedriver                        0x00000001063101f8 chromedriver + 4407800\n17  chromedriver                        0x000000010631037f chromedriver + 4408191\n18  chromedriver                        0x0000000106326cb5 chromedriver + 4500661\n19  libsystem_pthread.dylib             0x00007ff8167944e1 _pthread_start + 125\n20  libsystem_pthread.dylib             0x00007ff81678ff6b thread_start + 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#df = pd.DataFrame() # init dataframe to capture all scraped jobs # FIX: might not need\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# for each role title given above, return a list of up to 1,000 jobs from LinkedIn\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m data_roles:\n\u001b[0;32m----> 7\u001b[0m     job_links\u001b[38;5;241m.\u001b[39mappend(\u001b[43mscrape_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m20\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(job_links) # DEBUG\u001b[39;00m\n\u001b[1;32m     10\u001b[0m job_links_cleaned \u001b[38;5;241m=\u001b[39m clean_list(job_links) \u001b[38;5;66;03m# DUPLICATES remove dupliate links before scraping\u001b[39;00m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mscrape_links\u001b[0;34m(data_role)\u001b[0m\n\u001b[1;32m     13\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# SCROLL DOWN THE JOB LIST\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#driver.execute_script(\"window.scrollTo(0, document.body.div.jobs-search-results-list.scrollHeight);\")\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#left_rail_bottom = driver.find_element(By.CLASS_NAME, \"global-footer-compact__content.t-12.t-normal.text-align-center.clear-both.compactfooter-copyright\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#left_rail_bottom = driver.find_element(By.CSS_SELECTOR, \"li.global-footer-compact__item\")\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m left_rail_bottom \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompactfooter-about\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments[0].scrollIntoView();\u001b[39m\u001b[38;5;124m\"\u001b[39m, left_rail_bottom)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# scroll down for each job element\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#driver.execute_script(\"arguments[0].scrollIntoView();\", job)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Ironhackv1/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:857\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    854\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    855\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m value\n\u001b[0;32m--> 857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Ironhackv1/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:435\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    433\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(\n\u001b[1;32m    437\u001b[0m         response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Ironhackv1/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py:247\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    245\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"compactfooter-about\"]\"}\n  (Session info: chrome=104.0.5112.101)\nStacktrace:\n0   chromedriver                        0x000000010631f149 chromedriver + 4469065\n1   chromedriver                        0x00000001062a9233 chromedriver + 3985971\n2   chromedriver                        0x0000000105f3ffe8 chromedriver + 409576\n3   chromedriver                        0x0000000105f761d8 chromedriver + 631256\n4   chromedriver                        0x0000000105f76451 chromedriver + 631889\n5   chromedriver                        0x0000000105fa88f4 chromedriver + 837876\n6   chromedriver                        0x0000000105f93c8d chromedriver + 752781\n7   chromedriver                        0x0000000105fa6611 chromedriver + 828945\n8   chromedriver                        0x0000000105f93b53 chromedriver + 752467\n9   chromedriver                        0x0000000105f69905 chromedriver + 579845\n10  chromedriver                        0x0000000105f6a955 chromedriver + 584021\n11  chromedriver                        0x00000001062f06ad chromedriver + 4277933\n12  chromedriver                        0x00000001062f4b3a chromedriver + 4295482\n13  chromedriver                        0x00000001062f9cdf chromedriver + 4316383\n14  chromedriver                        0x00000001062f5857 chromedriver + 4298839\n15  chromedriver                        0x00000001062ce64f chromedriver + 4138575\n16  chromedriver                        0x00000001063101f8 chromedriver + 4407800\n17  chromedriver                        0x000000010631037f chromedriver + 4408191\n18  chromedriver                        0x0000000106326cb5 chromedriver + 4500661\n19  libsystem_pthread.dylib             0x00007ff8167944e1 _pthread_start + 125\n20  libsystem_pthread.dylib             0x00007ff81678ff6b thread_start + 15\n"
     ]
    }
   ],
   "source": [
    "data_roles = ['data analyst', 'data scientist', 'data architect'] # Titles taken from previous market analysis, used to capture all links to scrape with matching search term. # FIX: add all roles\n",
    "job_links = [] # init list to capture all job links\n",
    "#df = pd.DataFrame() # init dataframe to capture all scraped jobs # FIX: might not need\n",
    "\n",
    "# for each role title given above, return a list of up to 1,000 jobs from LinkedIn\n",
    "for title in data_roles:\n",
    "    job_links.append(scrape_links(title.replace(' ','%20')))\n",
    "#print(job_links) # DEBUG\n",
    "\n",
    "job_links_cleaned = clean_list(job_links) # DUPLICATES remove dupliate links before scraping\n",
    "\n",
    "# Scrape each new link\n",
    "#df = pd.concat([df, scrape_listing(job_links_cleaned)], axis=0).reset_index(drop=True) # FIX: might not need\n",
    "df = pd.DataFrame(scrape_listing(job_links_cleaned))#.reset_index(drop=True)\n",
    "driver.close()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1105f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47071d8",
   "metadata": {},
   "source": [
    "## Scrape Glassdoor using the Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search glassdoor for all job titles\n",
    "# save to glass_df: job title, salary range (or salary avg? both?)\n",
    "# add glassdoor_salary column to df\n",
    "# fill glassdoor_salary based on role name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7dcea2",
   "metadata": {},
   "source": [
    "## EDA and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# replace null values\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# given previous data sets, check salary info\n",
    "df.salary.unique()\n",
    "\n",
    "# remove duplicates\n",
    "clean.rm_dups(df)\n",
    "\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdba8e",
   "metadata": {},
   "source": [
    "## Export Data to CSV for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a52820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export df to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a0636",
   "metadata": {},
   "source": [
    "## Challenges & Lessons\n",
    "\n",
    "### Text extraction & different libraries: \n",
    "I couldn't extract text from my scraped LinkedIn data because I was try to pass data from one library's format (Selenium) into another library (Beautiful Soup). I restarted my kernel, rewrote my code (a lot), and one solution I found online used a function similar to others I had found. This seems to transfer text into a different format so that it's readable by other libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
